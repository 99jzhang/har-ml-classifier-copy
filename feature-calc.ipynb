{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt, welch, find_peaks\n",
    "import glob\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting pandas dataframes\n",
    "def plot_df(df):\n",
    "    # Plotting each column with row number as x-axis\n",
    "    df.plot()\n",
    "\n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Row Number')\n",
    "    plt.ylabel('Values')\n",
    "    plt.title('Pandas DataFrame Plot')\n",
    "    plt.show()\n",
    "\n",
    "def plot_ndarray(arr):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(arr, label='Values')\n",
    "    plt.legend()\n",
    "    plt.title('Autocorrelation')\n",
    "    plt.xlabel('Lag')\n",
    "    plt.ylabel('Autocorrelation value')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "# 2D Array, 53 features per row (after adjustments to features)\n",
    "all_features = np.empty((0, 53))\n",
    "# 1D array, 1 number per row\n",
    "all_answers = list()\n",
    "# List of every file name in RawData\n",
    "file_names = glob.glob(\"RawData\" + '/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psd(data, str):\n",
    "    # Calculate the Power Spectral Density (PSD) using Welch's method\n",
    "    frequencies, psd = welch(data, fs=50, nperseg=len(data))\n",
    "\n",
    "    # Find peaks\n",
    "    peaks, _ = find_peaks(psd)\n",
    "\n",
    "    # Extract first 3 peaks by height\n",
    "    sorted_peaks = np.argsort(psd[peaks])[-3:]\n",
    "    first_3_peaks = peaks[sorted_peaks]\n",
    "    first_3_peak_values = psd[first_3_peaks]\n",
    "    \n",
    "    # # Plotting\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "    # plt.semilogy(frequencies, psd, label='PSD')\n",
    "    # plt.plot(frequencies[first_3_peaks], first_3_peak_values, 'r^', label='Peaks')\n",
    "    # plt.title(f'Power Spectral Density (PSD) with Peaks of {str}')\n",
    "    # plt.xlabel('Frequency [Hz]')\n",
    "    # plt.ylabel('Power/Frequency [dB/Hz]')\n",
    "    # plt.grid(True)\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    # # Print the first 3 peaks and valleys along with their frequencies\n",
    "    # print(frequencies[first_3_peaks])\n",
    "    # print(first_3_peak_values)\n",
    "    # print(\"First 3 Peaks:\")\n",
    "    # for i, (freq, val) in enumerate(zip(frequencies[first_3_peaks], first_3_peak_values), 1):\n",
    "    #     print(f\"Peak {i}: Frequency = {freq:.2f} Hz, Value = {val:.2f} dB/Hz\")\n",
    "\n",
    "    # frequencies[first_3_peaks] is location (x-value of a graph), first_3_peak_values is like a y-value\n",
    "    return (frequencies[first_3_peaks], first_3_peak_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractWindow(acc_data, gyro_data, activityNum: int):\n",
    "    cur_features_row = list()\n",
    "\n",
    "    # Calculate mean for each direction in acc and gyro (6 total)\n",
    "    mean_acc_x = acc_data[0].mean()\n",
    "    mean_acc_y = acc_data[1].mean()\n",
    "    mean_acc_z = acc_data[2].mean()\n",
    "    mean_gyro_x = gyro_data[0].mean()\n",
    "    mean_gyro_y = gyro_data[1].mean()\n",
    "    mean_gyro_z = gyro_data[2].mean()\n",
    "    means = [mean_acc_x, mean_acc_y, mean_acc_z, mean_gyro_x, mean_gyro_y, mean_gyro_z]\n",
    "    for mean in means:\n",
    "        cur_features_row.append(mean)\n",
    "\n",
    "    # Calculate RMS for all (6 total numbers)\n",
    "    rms_acc_x = np.sqrt(np.mean(acc_data[0]**2))\n",
    "    rms_acc_y = np.sqrt(np.mean(acc_data[1]**2))\n",
    "    rms_acc_z = np.sqrt(np.mean(acc_data[2]**2))\n",
    "    rms_gyro_x = np.sqrt(np.mean(gyro_data[0]**2))\n",
    "    rms_gyro_y = np.sqrt(np.mean(gyro_data[1]**2))\n",
    "    rms_gyro_z = np.sqrt(np.mean(gyro_data[2]**2))\n",
    "    rmses = [rms_acc_x, rms_acc_y, rms_acc_z, rms_gyro_x, rms_gyro_y, rms_gyro_z]\n",
    "    for rms in rmses:\n",
    "        cur_features_row.append(rms)\n",
    "\n",
    "#========================================================================================================\n",
    "    # # No more autocorrelation\n",
    "    # composite_signal_series = pd.Series(acc_data[0])\n",
    "    # var = composite_signal_series.var()\n",
    "    # print(var)\n",
    "\n",
    "\n",
    "    # # Calculate autocorrelation for different lags\n",
    "    # lags = range(-100, 100)\n",
    "    # autocorr_values = [composite_signal_series.autocorr(lag=k) for k in lags]\n",
    "\n",
    "\n",
    "    # # Plotting the autocorrelation on the existing plot\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.plot(lags, autocorr_values, marker='o', label=activityNum)\n",
    "    # plt.axhline(0, color='black', linewidth=0.5)\n",
    "    # plt.title('Autocorrelation Comparison')\n",
    "    # plt.xlabel('Time lag (s)')\n",
    "    # plt.ylabel('Correlation')\n",
    "    # plt.grid(True)\n",
    "    # plt.legend(loc='best')\n",
    "    # plt.show()\n",
    "#=================================================================================================================\n",
    "\n",
    "    # Spectral peaks (36)\n",
    "    multi_tuples = list()\n",
    "    multi_tuples.append(psd(acc_data[0], \"Acc_X\"))\n",
    "    multi_tuples.append(psd(acc_data[1], \"Acc_Y\"))\n",
    "    multi_tuples.append(psd(acc_data[2], \"Acc_Z\"))\n",
    "    multi_tuples.append(psd(gyro_data[0], \"Gyro_X\"))\n",
    "    multi_tuples.append(psd(gyro_data[1], \"Gyro_Y\"))\n",
    "    multi_tuples.append(psd(gyro_data[2], \"Gyro_Z\"))\n",
    "\n",
    "    # Adds all 3 positions of the peak, then all 3 values of the peak\n",
    "    for tuple in multi_tuples:\n",
    "        for arr in tuple:\n",
    "            for data_pt in arr:\n",
    "                cur_features_row.append(data_pt)\n",
    "\n",
    "    # Resultant/magnitude acceleration (1) \n",
    "    acc_mag = np.sqrt(mean_acc_x**2 + mean_acc_y**2 + mean_acc_z**2)\n",
    "    cur_features_row.append(acc_mag)\n",
    "\n",
    "    # Resultant gyro (1)\n",
    "    gyro_mag = np.sqrt(mean_gyro_x**2 + mean_gyro_y**2 + mean_gyro_z**2)\n",
    "    cur_features_row.append(gyro_mag)\n",
    "\n",
    "    # Angle btwn resultant acc and each acc input (3)\n",
    "    angle_accX = np.degrees(np.arccos(mean_acc_x / acc_mag))\n",
    "    angle_accY = np.degrees(np.arccos(mean_acc_y / acc_mag))\n",
    "    angle_accZ = np.degrees(np.arccos(mean_acc_z / acc_mag))\n",
    "    cur_features_row.append(angle_accX)\n",
    "    cur_features_row.append(angle_accY)\n",
    "    cur_features_row.append(angle_accZ)\n",
    "\n",
    "\n",
    "    cur_features_row_as_npArr = np.array(cur_features_row)\n",
    "\n",
    "    global all_features\n",
    "    all_features = np.vstack((all_features, cur_features_row_as_npArr))\n",
    "    all_answers.append(activityNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''These are for noise reduction'''\n",
    "# Function to apply median filter\n",
    "def apply_median_filter(data, window_size):\n",
    "    return data.rolling(window=window_size, center=True, min_periods=1).median()\n",
    "\n",
    "# Function to design a low-pass Butterworth filter\n",
    "def butter_lowpass(cutoff, fs, order):\n",
    "    nyq = 0.5 * fs  # Nyquist frequency\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "# Function to apply the Butterworth filter\n",
    "def apply_low_butter(data, cutoff, fs, order):\n",
    "    b, a = butter_lowpass(cutoff, fs, order)\n",
    "    y = filtfilt(b, a, data, axis=0)\n",
    "    return y\n",
    "\n",
    "'''This is for getting body acc component from total acc'''\n",
    "# Define a function to create a high-pass Butterworth filter\n",
    "def butter_highpass(cutoff, fs, order):\n",
    "    nyquist = 0.5 * fs  # Nyquist frequency is half the sampling rate\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    return b, a\n",
    "\n",
    "# Define a function to apply the high-pass filter to data\n",
    "def apply_high_butter(data, cutoff, fs, order):\n",
    "    b, a = butter_highpass(cutoff, fs, order)\n",
    "    y = filtfilt(b, a, data, axis=0)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For testing'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractAllFeatures(label: str):\n",
    "    splitted = label.split()\n",
    "    experimentNum, activityNum, start, end = int(splitted[0]), int(splitted[2]), int(splitted[3]), int(splitted[4])\n",
    "\n",
    "    acc_file_name = file_names[experimentNum-1]\n",
    "    gyro_file_name = file_names[experimentNum-1+61]\n",
    "\n",
    "    # Reading acc and gyro in data for this label range\n",
    "    acc_data = pd.read_csv(acc_file_name, sep=' ', header=None, skiprows=start, nrows=end-start)\n",
    "    gyro_data = pd.read_csv(gyro_file_name, sep=' ', header=None, skiprows=start, nrows=end-start)\n",
    "\n",
    "    ''' I'm not too sure how these filters work but I hope they do'''\n",
    "    # Apply median filter to each axis of accelerometer and gyroscope data (for noise reduction)\n",
    "    WINDOW_SIZE = 3\n",
    "    acc_data_filtered = acc_data.apply(apply_median_filter, window_size=WINDOW_SIZE)\n",
    "    gyro_data_filtered = gyro_data.apply(apply_median_filter, window_size=WINDOW_SIZE)\n",
    "    \n",
    "    # Apply low-pass Butterworth filter to each axis of accelerometer and gyroscope data (for noise reduction)\n",
    "    FS = 50                     # Sampling frequency in Hz (you mentioned 50Hz)\n",
    "    CUTOFF_LOW_PASS = 20        # Desired cutoff frequency of the filter, Hz\n",
    "    ORDER = 3                   # Order of the Butterworth filter\n",
    "    acc_data_filtered = acc_data_filtered.apply(lambda col: apply_low_butter(col, CUTOFF_LOW_PASS, FS, ORDER))\n",
    "    gyro_data_filtered = gyro_data_filtered.apply(lambda col: apply_low_butter(col, CUTOFF_LOW_PASS, FS, ORDER))    \n",
    "\n",
    "    # Apply high-pass Butterworth filter to acc to get body component\n",
    "    CUTOFF_HIGH_PASS = 0.3\n",
    "    acc_data_filtered = acc_data_filtered.apply(lambda col: apply_low_butter(col, CUTOFF_HIGH_PASS, FS, ORDER))\n",
    "\n",
    "    # 2.56sec windows * 50 samples per sec = 128 samples per window\n",
    "    # 50% overlap means we go up by 64 for every new window\n",
    "    # end-64 is used to skip the last half window that gets added\n",
    "    length = end-start\n",
    "    i = 0\n",
    "    while i < length-64:\n",
    "        trueEnd = min(i+128, length)\n",
    "        extractWindow(acc_data_filtered[i: trueEnd], gyro_data_filtered[i: trueEnd], activityNum)\n",
    "        i += 64\n",
    "\n",
    "\n",
    "'''For testing'''\n",
    "# extractAllFeatures('1 1 5 250 1232')\n",
    "# extractAllFeatures('1 1 7 1233 1392')\n",
    "# extractAllFeatures('1 1 4 1393 2194')\n",
    "# extractAllFeatures('1 1 1 7496 8078')\n",
    "# extractAllFeatures('1 1 2 14069 14699')\n",
    "# extractAllFeatures('1 1 3 14869 15492')\n",
    "\n",
    "# extractAllFeatures('1 1 1 7496 8078')\n",
    "# extractAllFeatures('1 1 2 14069 14699')\n",
    "# extractAllFeatures('1 1 3 14869 15492')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 53 and the array at index 1 has size 35",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 29\u001b[0m\n\u001b[1;32m     24\u001b[0m     np\u001b[38;5;241m.\u001b[39msavetxt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_test.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_test, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# For each section: \u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# i. Load that section's respective data\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# ii. For each window in that section:\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# a. Apply noise filters on the window\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# b. Compute features from the window\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m# c. Store those feature values in an array\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels_list:\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mextractAllFeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''I checked: all_features and all_answers are the same length (same number of rows)'''\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Take all_features and answers, split into test and train, and finally write to .txt file\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 36\u001b[0m, in \u001b[0;36mextractAllFeatures\u001b[0;34m(label)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m length\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m64\u001b[39m:\n\u001b[1;32m     35\u001b[0m     trueEnd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m128\u001b[39m, length)\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mextractWindow\u001b[49m\u001b[43m(\u001b[49m\u001b[43macc_data_filtered\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrueEnd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgyro_data_filtered\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrueEnd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivityNum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n",
      "Cell \u001b[0;32mIn[5], line 85\u001b[0m, in \u001b[0;36mextractWindow\u001b[0;34m(acc_data, gyro_data, activityNum)\u001b[0m\n\u001b[1;32m     82\u001b[0m cur_features_row_as_npArr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(cur_features_row)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m all_features\n\u001b[0;32m---> 85\u001b[0m all_features \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_features_row_as_npArr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m all_answers\u001b[38;5;241m.\u001b[39mappend(activityNum)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/numpy/core/shape_base.py:289\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    288\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 53 and the array at index 1 has size 35"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    # Grab the labels.txt file telling you what each data section means\n",
    "    labels_file = open('labels.txt') \n",
    "    labels_list = labels_file.readlines()\n",
    "\n",
    "    # For each section: \n",
    "        # i. Load that section's respective data\n",
    "        # ii. For each window in that section:\n",
    "            # a. Apply noise filters on the window\n",
    "            # b. Compute features from the window\n",
    "            # c. Store those feature values in an array\n",
    "    for label in labels_list:\n",
    "        extractAllFeatures(label)\n",
    "    \n",
    "    '''I checked: all_features and all_answers are the same length (same number of rows)'''\n",
    "\n",
    "    # Take all_features and answers, split into test and train, and finally write to .txt file\n",
    "    X_train, X_test, y_train, y_test = train_test_split(all_features, all_answers, test_size=0.3, random_state=42)\n",
    "    \n",
    "    np.savetxt(\"X_train.txt\", X_train, delimiter=' ', fmt='%.8f')\n",
    "    np.savetxt(\"X_test.txt\", X_test, delimiter=' ', fmt='%.8f')\n",
    "    np.savetxt(\"y_train.txt\", y_train, delimiter=' ', fmt='%d')\n",
    "    np.savetxt(\"y_test.txt\", y_test, delimiter=' ', fmt='%d')\n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
