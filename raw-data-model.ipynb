{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import time \n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_PATH = './data/'\n",
    "DATASET_PATH = DATA_PATH + 'uci-data/'\n",
    "MODELS_PATH = DATA_PATH + 'models/raw-models/'\n",
    "FEATURES = ['accX', 'accY', 'accZ', 'gyroX', 'gyroY', 'gyroZ', \"subject\", \"activity\"]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SAMPLE_SIZE = BATCH_SIZE * 1500\n",
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/lt/w0169b7x5ml3psz3nly9vj3m0000gn/T/ipykernel_43370/1212202754.py:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  '''\n",
      "/var/folders/lt/w0169b7x5ml3psz3nly9vj3m0000gn/T/ipykernel_43370/1212202754.py:9: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  complete_set = pd.read_csv(DATA_PATH + \"self-calculated/raw-data.txt\", sep='\\s+', header=None)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Load the data\n",
    "train_set = pd.read_csv(DATA_PATH + \"self-calculated/raw-train-data.txt\", sep='\\s+', header=None)\n",
    "test_set = pd.read_csv(DATA_PATH + \"self-calculated/raw-test-data.txt\", sep='\\s+', header=None)\n",
    "\n",
    "train_set.columns=FEATURES\n",
    "test_set.columns=FEATURES\n",
    "'''\n",
    "complete_set = pd.read_csv(DATA_PATH + \"self-calculated/raw-data.txt\", sep='\\s+', header=None)\n",
    "complete_set.columns = FEATURES\n",
    "train_set, test_set = train_test_split(complete_set, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model\n",
    "device = None\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "class RawDataModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sequential_module = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=7, out_channels=256, kernel_size=1),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool1d(1),\n",
    "            nn.BatchNorm1d(256),\n",
    "\n",
    "            nn.Conv1d(in_channels=256, out_channels=128, kernel_size=1),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool1d(1),\n",
    "            nn.BatchNorm1d(128),\n",
    "\n",
    "            nn.Conv1d(in_channels=128, out_channels=64, kernel_size=1),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool1d(1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(64, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Linear(128, 12),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential_module(x)\n",
    "    \n",
    "model = RawDataModel().to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing function\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    # Get batch num\n",
    "    num_batches = len(dataloader.dataset) / BATCH_SIZE\n",
    "    i = 0\n",
    "\n",
    "    # Set the model to train mode\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.unsqueeze(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        i += 1\n",
    "        if batch % 50 == 0:\n",
    "            print(f\"loss: {loss.item()}, batch: {i} out of {math.ceil(num_batches)}\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.unsqueeze(-1)\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    return(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset preparation\n",
    "class HAPTDataset(Dataset):\n",
    "    def __init__(self, dataset, features, label):\n",
    "        self.data = torch.tensor(dataset[features].values, dtype=torch.float32)[:SAMPLE_SIZE]\n",
    "        self.labels = torch.tensor(dataset[label].values, dtype=torch.float32)[:SAMPLE_SIZE]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "    \n",
    "train_dataset = HAPTDataset(train_set, train_set.columns[:-1], 'activity')\n",
    "test_dataset = HAPTDataset(test_set, test_set.columns[:-1], 'activity')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.515962600708008, batch: 1 out of 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.419985771179199, batch: 51 out of 1500\n",
      "loss: 2.354149341583252, batch: 101 out of 1500\n",
      "loss: 2.2254412174224854, batch: 151 out of 1500\n",
      "loss: 2.237992286682129, batch: 201 out of 1500\n",
      "loss: 2.153644561767578, batch: 251 out of 1500\n",
      "loss: 2.0862958431243896, batch: 301 out of 1500\n",
      "loss: 2.0832762718200684, batch: 351 out of 1500\n",
      "loss: 1.9158828258514404, batch: 401 out of 1500\n",
      "loss: 1.984983205795288, batch: 451 out of 1500\n",
      "loss: 2.1097092628479004, batch: 501 out of 1500\n",
      "loss: 1.9958930015563965, batch: 551 out of 1500\n",
      "loss: 2.064727783203125, batch: 601 out of 1500\n",
      "loss: 2.021512985229492, batch: 651 out of 1500\n",
      "loss: 2.0890917778015137, batch: 701 out of 1500\n",
      "loss: 2.0066006183624268, batch: 751 out of 1500\n",
      "loss: 2.108241558074951, batch: 801 out of 1500\n",
      "loss: 2.1001148223876953, batch: 851 out of 1500\n",
      "loss: 2.0084948539733887, batch: 901 out of 1500\n",
      "loss: 2.1027328968048096, batch: 951 out of 1500\n",
      "loss: 2.045600175857544, batch: 1001 out of 1500\n",
      "loss: 2.001514434814453, batch: 1051 out of 1500\n",
      "loss: 1.9542208909988403, batch: 1101 out of 1500\n",
      "loss: 1.9990389347076416, batch: 1151 out of 1500\n",
      "loss: 1.9857945442199707, batch: 1201 out of 1500\n",
      "loss: 1.9802104234695435, batch: 1251 out of 1500\n",
      "loss: 1.9387636184692383, batch: 1301 out of 1500\n",
      "loss: 2.096965789794922, batch: 1351 out of 1500\n",
      "loss: 2.037177562713623, batch: 1401 out of 1500\n",
      "loss: 1.972259759902954, batch: 1451 out of 1500\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.0256683826446533, batch: 1 out of 1500\n",
      "loss: 2.022312879562378, batch: 51 out of 1500\n",
      "loss: 2.008498191833496, batch: 101 out of 1500\n",
      "loss: 2.027888536453247, batch: 151 out of 1500\n",
      "loss: 2.0325043201446533, batch: 201 out of 1500\n",
      "loss: 1.9509608745574951, batch: 251 out of 1500\n",
      "loss: 1.8960349559783936, batch: 301 out of 1500\n",
      "loss: 1.9524998664855957, batch: 351 out of 1500\n",
      "loss: 2.068533420562744, batch: 401 out of 1500\n",
      "loss: 1.9366270303726196, batch: 451 out of 1500\n",
      "loss: 1.8430414199829102, batch: 501 out of 1500\n",
      "loss: 1.9965404272079468, batch: 551 out of 1500\n",
      "loss: 1.9508774280548096, batch: 601 out of 1500\n",
      "loss: 1.9210126399993896, batch: 651 out of 1500\n",
      "loss: 1.8984711170196533, batch: 701 out of 1500\n",
      "loss: 1.8552712202072144, batch: 751 out of 1500\n",
      "loss: 1.9099006652832031, batch: 801 out of 1500\n",
      "loss: 1.911116600036621, batch: 851 out of 1500\n",
      "loss: 1.929291009902954, batch: 901 out of 1500\n",
      "loss: 1.9181034564971924, batch: 951 out of 1500\n",
      "loss: 1.8260114192962646, batch: 1001 out of 1500\n",
      "loss: 2.0337743759155273, batch: 1051 out of 1500\n",
      "loss: 1.8862123489379883, batch: 1101 out of 1500\n",
      "loss: 1.996004581451416, batch: 1151 out of 1500\n",
      "loss: 1.9489599466323853, batch: 1201 out of 1500\n",
      "loss: 1.9082705974578857, batch: 1251 out of 1500\n",
      "loss: 1.9085736274719238, batch: 1301 out of 1500\n",
      "loss: 1.9567121267318726, batch: 1351 out of 1500\n",
      "loss: 1.9859219789505005, batch: 1401 out of 1500\n",
      "loss: 1.8796563148498535, batch: 1451 out of 1500\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.9072250127792358, batch: 1 out of 1500\n",
      "loss: 1.8619661331176758, batch: 51 out of 1500\n",
      "loss: 1.8838796615600586, batch: 101 out of 1500\n",
      "loss: 2.013800621032715, batch: 151 out of 1500\n",
      "loss: 1.857271671295166, batch: 201 out of 1500\n",
      "loss: 1.9524710178375244, batch: 251 out of 1500\n",
      "loss: 1.9414316415786743, batch: 301 out of 1500\n",
      "loss: 1.8198047876358032, batch: 351 out of 1500\n",
      "loss: 1.969669222831726, batch: 401 out of 1500\n",
      "loss: 1.949489712715149, batch: 451 out of 1500\n",
      "loss: 1.9241560697555542, batch: 501 out of 1500\n",
      "loss: 1.9349658489227295, batch: 551 out of 1500\n",
      "loss: 1.7662017345428467, batch: 601 out of 1500\n",
      "loss: 2.0545613765716553, batch: 651 out of 1500\n",
      "loss: 1.8816217184066772, batch: 701 out of 1500\n",
      "loss: 2.0476179122924805, batch: 751 out of 1500\n",
      "loss: 1.854804277420044, batch: 801 out of 1500\n",
      "loss: 1.9130349159240723, batch: 851 out of 1500\n",
      "loss: 1.911596417427063, batch: 901 out of 1500\n",
      "loss: 1.88492751121521, batch: 951 out of 1500\n",
      "loss: 2.0393118858337402, batch: 1001 out of 1500\n",
      "loss: 1.817476749420166, batch: 1051 out of 1500\n",
      "loss: 1.8804950714111328, batch: 1101 out of 1500\n",
      "loss: 1.9919445514678955, batch: 1151 out of 1500\n",
      "loss: 1.9283971786499023, batch: 1201 out of 1500\n",
      "loss: 1.9965687990188599, batch: 1251 out of 1500\n",
      "loss: 1.714045524597168, batch: 1301 out of 1500\n",
      "loss: 1.959457516670227, batch: 1351 out of 1500\n",
      "loss: 1.8564441204071045, batch: 1401 out of 1500\n",
      "loss: 2.088469982147217, batch: 1451 out of 1500\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.9310131072998047, batch: 1 out of 1500\n",
      "loss: 2.0425612926483154, batch: 51 out of 1500\n",
      "loss: 2.001249313354492, batch: 101 out of 1500\n",
      "loss: 1.9550535678863525, batch: 151 out of 1500\n",
      "loss: 1.9794329404830933, batch: 201 out of 1500\n",
      "loss: 1.8056786060333252, batch: 251 out of 1500\n",
      "loss: 1.9956989288330078, batch: 301 out of 1500\n",
      "loss: 2.0391297340393066, batch: 351 out of 1500\n",
      "loss: 1.9174034595489502, batch: 401 out of 1500\n",
      "loss: 1.965341329574585, batch: 451 out of 1500\n",
      "loss: 1.9797759056091309, batch: 501 out of 1500\n",
      "loss: 2.0293238162994385, batch: 551 out of 1500\n",
      "loss: 1.8676528930664062, batch: 601 out of 1500\n",
      "loss: 1.790628433227539, batch: 651 out of 1500\n",
      "loss: 1.9356813430786133, batch: 701 out of 1500\n",
      "loss: 1.8946799039840698, batch: 751 out of 1500\n",
      "loss: 2.009216070175171, batch: 801 out of 1500\n",
      "loss: 1.815301537513733, batch: 851 out of 1500\n",
      "loss: 2.039694309234619, batch: 901 out of 1500\n",
      "loss: 1.833594560623169, batch: 951 out of 1500\n",
      "loss: 1.9372293949127197, batch: 1001 out of 1500\n",
      "loss: 1.8386589288711548, batch: 1051 out of 1500\n",
      "loss: 1.9933247566223145, batch: 1101 out of 1500\n",
      "loss: 1.8399354219436646, batch: 1151 out of 1500\n",
      "loss: 1.8928089141845703, batch: 1201 out of 1500\n",
      "loss: 2.020085096359253, batch: 1251 out of 1500\n",
      "loss: 1.9738271236419678, batch: 1301 out of 1500\n",
      "loss: 1.9358528852462769, batch: 1351 out of 1500\n",
      "loss: 2.0057129859924316, batch: 1401 out of 1500\n",
      "loss: 1.9365756511688232, batch: 1451 out of 1500\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.83603835105896, batch: 1 out of 1500\n",
      "loss: 1.6834208965301514, batch: 51 out of 1500\n",
      "loss: 2.128613233566284, batch: 101 out of 1500\n",
      "loss: 1.9720795154571533, batch: 151 out of 1500\n",
      "loss: 1.7957556247711182, batch: 201 out of 1500\n",
      "loss: 1.96004319190979, batch: 251 out of 1500\n",
      "loss: 1.80317223072052, batch: 301 out of 1500\n",
      "loss: 1.9505198001861572, batch: 351 out of 1500\n",
      "loss: 2.162841796875, batch: 401 out of 1500\n",
      "loss: 1.8453813791275024, batch: 451 out of 1500\n",
      "loss: 1.9783694744110107, batch: 501 out of 1500\n",
      "loss: 1.863389253616333, batch: 551 out of 1500\n",
      "loss: 1.9500617980957031, batch: 601 out of 1500\n",
      "loss: 1.9303669929504395, batch: 651 out of 1500\n",
      "loss: 1.8990750312805176, batch: 701 out of 1500\n",
      "loss: 1.9064064025878906, batch: 751 out of 1500\n",
      "loss: 1.7591307163238525, batch: 801 out of 1500\n",
      "loss: 2.0738816261291504, batch: 851 out of 1500\n",
      "loss: 1.8530867099761963, batch: 901 out of 1500\n",
      "loss: 1.9780021905899048, batch: 951 out of 1500\n",
      "loss: 2.0009608268737793, batch: 1001 out of 1500\n",
      "loss: 1.9333899021148682, batch: 1051 out of 1500\n",
      "loss: 2.0514869689941406, batch: 1101 out of 1500\n",
      "loss: 1.900173544883728, batch: 1151 out of 1500\n",
      "loss: 1.8925226926803589, batch: 1201 out of 1500\n",
      "loss: 1.9452824592590332, batch: 1251 out of 1500\n",
      "loss: 1.7962651252746582, batch: 1301 out of 1500\n",
      "loss: 1.9788737297058105, batch: 1351 out of 1500\n",
      "loss: 1.9481639862060547, batch: 1401 out of 1500\n",
      "loss: 2.108156681060791, batch: 1451 out of 1500\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.888702630996704, batch: 1 out of 1500\n",
      "loss: 2.0210611820220947, batch: 51 out of 1500\n",
      "loss: 1.9370551109313965, batch: 101 out of 1500\n",
      "loss: 1.9356669187545776, batch: 151 out of 1500\n",
      "loss: 1.825157880783081, batch: 201 out of 1500\n",
      "loss: 1.8761634826660156, batch: 251 out of 1500\n",
      "loss: 1.8927130699157715, batch: 301 out of 1500\n",
      "loss: 1.8895950317382812, batch: 351 out of 1500\n",
      "loss: 1.9041006565093994, batch: 401 out of 1500\n",
      "loss: 1.935306429862976, batch: 451 out of 1500\n",
      "loss: 2.005796432495117, batch: 501 out of 1500\n",
      "loss: 1.887073278427124, batch: 551 out of 1500\n",
      "loss: 1.879889726638794, batch: 601 out of 1500\n",
      "loss: 1.8774628639221191, batch: 651 out of 1500\n",
      "loss: 1.9213929176330566, batch: 701 out of 1500\n",
      "loss: 1.9359087944030762, batch: 751 out of 1500\n",
      "loss: 2.0895986557006836, batch: 801 out of 1500\n",
      "loss: 1.7777760028839111, batch: 851 out of 1500\n",
      "loss: 1.7665891647338867, batch: 901 out of 1500\n",
      "loss: 1.831603765487671, batch: 951 out of 1500\n",
      "loss: 1.7954391241073608, batch: 1001 out of 1500\n",
      "loss: 1.9526469707489014, batch: 1051 out of 1500\n",
      "loss: 1.9514073133468628, batch: 1101 out of 1500\n",
      "loss: 1.6667594909667969, batch: 1151 out of 1500\n",
      "loss: 1.9122216701507568, batch: 1201 out of 1500\n",
      "loss: 1.9241366386413574, batch: 1251 out of 1500\n",
      "loss: 2.0421102046966553, batch: 1301 out of 1500\n",
      "loss: 1.742997169494629, batch: 1351 out of 1500\n",
      "loss: 1.8221226930618286, batch: 1401 out of 1500\n",
      "loss: 1.955711841583252, batch: 1451 out of 1500\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.9501779079437256, batch: 1 out of 1500\n",
      "loss: 1.9337472915649414, batch: 51 out of 1500\n",
      "loss: 1.9675631523132324, batch: 101 out of 1500\n",
      "loss: 1.9663560390472412, batch: 151 out of 1500\n",
      "loss: 2.06001877784729, batch: 201 out of 1500\n",
      "loss: 1.9710273742675781, batch: 251 out of 1500\n",
      "loss: 2.0741066932678223, batch: 301 out of 1500\n",
      "loss: 1.8429677486419678, batch: 351 out of 1500\n",
      "loss: 1.760779857635498, batch: 401 out of 1500\n",
      "loss: 1.9796996116638184, batch: 451 out of 1500\n",
      "loss: 1.7515439987182617, batch: 501 out of 1500\n",
      "loss: 1.8913500308990479, batch: 551 out of 1500\n",
      "loss: 1.8806039094924927, batch: 601 out of 1500\n",
      "loss: 1.929152011871338, batch: 651 out of 1500\n",
      "loss: 1.9255468845367432, batch: 701 out of 1500\n",
      "loss: 1.8921797275543213, batch: 751 out of 1500\n",
      "loss: 1.836519718170166, batch: 801 out of 1500\n",
      "loss: 1.9144296646118164, batch: 851 out of 1500\n",
      "loss: 2.0184881687164307, batch: 901 out of 1500\n",
      "loss: 1.803192138671875, batch: 951 out of 1500\n",
      "loss: 2.156125545501709, batch: 1001 out of 1500\n",
      "loss: 1.929962396621704, batch: 1051 out of 1500\n",
      "loss: 1.907199740409851, batch: 1101 out of 1500\n",
      "loss: 1.8002750873565674, batch: 1151 out of 1500\n",
      "loss: 1.9217430353164673, batch: 1201 out of 1500\n",
      "loss: 2.0217251777648926, batch: 1251 out of 1500\n",
      "loss: 1.813255786895752, batch: 1301 out of 1500\n",
      "loss: 1.7546212673187256, batch: 1351 out of 1500\n",
      "loss: 1.914413571357727, batch: 1401 out of 1500\n",
      "loss: 1.934751033782959, batch: 1451 out of 1500\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.7875070571899414, batch: 1 out of 1500\n",
      "loss: 1.8726837635040283, batch: 51 out of 1500\n",
      "loss: 1.8255213499069214, batch: 101 out of 1500\n",
      "loss: 1.8545657396316528, batch: 151 out of 1500\n",
      "loss: 1.9021583795547485, batch: 201 out of 1500\n",
      "loss: 1.8967983722686768, batch: 251 out of 1500\n",
      "loss: 1.8744076490402222, batch: 301 out of 1500\n",
      "loss: 1.7514232397079468, batch: 351 out of 1500\n",
      "loss: 1.8218133449554443, batch: 401 out of 1500\n",
      "loss: 1.9102058410644531, batch: 451 out of 1500\n",
      "loss: 1.905941367149353, batch: 501 out of 1500\n",
      "loss: 1.8845117092132568, batch: 551 out of 1500\n",
      "loss: 1.832911491394043, batch: 601 out of 1500\n",
      "loss: 1.8698965311050415, batch: 651 out of 1500\n",
      "loss: 1.8804070949554443, batch: 701 out of 1500\n",
      "loss: 1.8546111583709717, batch: 751 out of 1500\n",
      "loss: 1.8713507652282715, batch: 801 out of 1500\n",
      "loss: 1.9667503833770752, batch: 851 out of 1500\n",
      "loss: 1.9982836246490479, batch: 901 out of 1500\n",
      "loss: 1.8549654483795166, batch: 951 out of 1500\n",
      "loss: 1.7522814273834229, batch: 1001 out of 1500\n",
      "loss: 1.9631528854370117, batch: 1051 out of 1500\n",
      "loss: 1.929000735282898, batch: 1101 out of 1500\n",
      "loss: 1.9779044389724731, batch: 1151 out of 1500\n",
      "loss: 1.863451361656189, batch: 1201 out of 1500\n",
      "loss: 2.000102996826172, batch: 1251 out of 1500\n",
      "loss: 1.9242336750030518, batch: 1301 out of 1500\n",
      "loss: 1.8517379760742188, batch: 1351 out of 1500\n",
      "loss: 1.8087530136108398, batch: 1401 out of 1500\n",
      "loss: 1.832662582397461, batch: 1451 out of 1500\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.024960517883301, batch: 1 out of 1500\n",
      "loss: 1.8127925395965576, batch: 51 out of 1500\n",
      "loss: 1.9044651985168457, batch: 101 out of 1500\n",
      "loss: 1.975386381149292, batch: 151 out of 1500\n",
      "loss: 1.8843979835510254, batch: 201 out of 1500\n",
      "loss: 1.950709581375122, batch: 251 out of 1500\n",
      "loss: 2.0161824226379395, batch: 301 out of 1500\n",
      "loss: 2.0199294090270996, batch: 351 out of 1500\n",
      "loss: 1.7783176898956299, batch: 401 out of 1500\n",
      "loss: 1.933922290802002, batch: 451 out of 1500\n",
      "loss: 1.8847270011901855, batch: 501 out of 1500\n",
      "loss: 1.8931121826171875, batch: 551 out of 1500\n",
      "loss: 1.7495033740997314, batch: 601 out of 1500\n",
      "loss: 2.0774612426757812, batch: 651 out of 1500\n",
      "loss: 1.9771034717559814, batch: 701 out of 1500\n",
      "loss: 1.9353265762329102, batch: 751 out of 1500\n",
      "loss: 1.9802227020263672, batch: 801 out of 1500\n",
      "loss: 1.8841756582260132, batch: 851 out of 1500\n",
      "loss: 1.7650927305221558, batch: 901 out of 1500\n",
      "loss: 1.7900587320327759, batch: 951 out of 1500\n",
      "loss: 1.810163974761963, batch: 1001 out of 1500\n",
      "loss: 1.915891408920288, batch: 1051 out of 1500\n",
      "loss: 1.8696045875549316, batch: 1101 out of 1500\n",
      "loss: 1.7145851850509644, batch: 1151 out of 1500\n",
      "loss: 1.8184466361999512, batch: 1201 out of 1500\n",
      "loss: 1.88935387134552, batch: 1251 out of 1500\n",
      "loss: 1.9472935199737549, batch: 1301 out of 1500\n",
      "loss: 1.6653094291687012, batch: 1351 out of 1500\n",
      "loss: 1.7965906858444214, batch: 1401 out of 1500\n",
      "loss: 2.105074882507324, batch: 1451 out of 1500\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.006244421005249, batch: 1 out of 1500\n",
      "loss: 1.861268162727356, batch: 51 out of 1500\n",
      "loss: 1.997017502784729, batch: 101 out of 1500\n",
      "loss: 1.9354115724563599, batch: 151 out of 1500\n",
      "loss: 1.934197187423706, batch: 201 out of 1500\n",
      "loss: 1.8848631381988525, batch: 251 out of 1500\n",
      "loss: 1.9132896661758423, batch: 301 out of 1500\n",
      "loss: 1.9391502141952515, batch: 351 out of 1500\n",
      "loss: 1.794845700263977, batch: 401 out of 1500\n",
      "loss: 1.97171950340271, batch: 451 out of 1500\n",
      "loss: 1.9299649000167847, batch: 501 out of 1500\n",
      "loss: 1.8011529445648193, batch: 551 out of 1500\n",
      "loss: 1.911900281906128, batch: 601 out of 1500\n",
      "loss: 1.7668527364730835, batch: 651 out of 1500\n",
      "loss: 1.9704838991165161, batch: 701 out of 1500\n",
      "loss: 1.9430339336395264, batch: 751 out of 1500\n",
      "loss: 2.021758794784546, batch: 801 out of 1500\n",
      "loss: 1.9098081588745117, batch: 851 out of 1500\n",
      "loss: 1.7176940441131592, batch: 901 out of 1500\n",
      "loss: 2.004429817199707, batch: 951 out of 1500\n",
      "loss: 1.8639843463897705, batch: 1001 out of 1500\n",
      "loss: 1.9408202171325684, batch: 1051 out of 1500\n",
      "loss: 1.7955737113952637, batch: 1101 out of 1500\n",
      "loss: 1.9026570320129395, batch: 1151 out of 1500\n",
      "loss: 2.0138309001922607, batch: 1201 out of 1500\n",
      "loss: 1.7987215518951416, batch: 1251 out of 1500\n",
      "loss: 1.9012693166732788, batch: 1301 out of 1500\n",
      "loss: 2.112084150314331, batch: 1351 out of 1500\n",
      "loss: 1.9408459663391113, batch: 1401 out of 1500\n",
      "loss: 1.9895985126495361, batch: 1451 out of 1500\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.809309959411621, batch: 1 out of 1500\n",
      "loss: 1.8723278045654297, batch: 51 out of 1500\n",
      "loss: 1.815006971359253, batch: 101 out of 1500\n",
      "loss: 1.9565629959106445, batch: 151 out of 1500\n",
      "loss: 1.8811604976654053, batch: 201 out of 1500\n",
      "loss: 1.8833627700805664, batch: 251 out of 1500\n",
      "loss: 1.9342708587646484, batch: 301 out of 1500\n",
      "loss: 1.9281666278839111, batch: 351 out of 1500\n",
      "loss: 1.894092082977295, batch: 401 out of 1500\n",
      "loss: 1.915384292602539, batch: 451 out of 1500\n",
      "loss: 2.0261685848236084, batch: 501 out of 1500\n",
      "loss: 1.794488549232483, batch: 551 out of 1500\n",
      "loss: 1.9113414287567139, batch: 601 out of 1500\n",
      "loss: 1.7933263778686523, batch: 651 out of 1500\n",
      "loss: 1.9337985515594482, batch: 701 out of 1500\n",
      "loss: 1.9245487451553345, batch: 751 out of 1500\n",
      "loss: 2.0894012451171875, batch: 801 out of 1500\n",
      "loss: 1.8138689994812012, batch: 851 out of 1500\n",
      "loss: 1.7154061794281006, batch: 901 out of 1500\n",
      "loss: 1.897640347480774, batch: 951 out of 1500\n",
      "loss: 1.8807249069213867, batch: 1001 out of 1500\n",
      "loss: 1.9357248544692993, batch: 1051 out of 1500\n",
      "loss: 1.7555378675460815, batch: 1101 out of 1500\n",
      "loss: 1.7530533075332642, batch: 1151 out of 1500\n",
      "loss: 1.9788308143615723, batch: 1201 out of 1500\n",
      "loss: 2.003213882446289, batch: 1251 out of 1500\n",
      "loss: 1.776968002319336, batch: 1301 out of 1500\n",
      "loss: 2.035646677017212, batch: 1351 out of 1500\n",
      "loss: 1.9707685708999634, batch: 1401 out of 1500\n",
      "loss: 1.8932182788848877, batch: 1451 out of 1500\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.8791213035583496, batch: 1 out of 1500\n",
      "loss: 1.918515682220459, batch: 51 out of 1500\n",
      "loss: 1.78265380859375, batch: 101 out of 1500\n",
      "loss: 1.7964682579040527, batch: 151 out of 1500\n",
      "loss: 1.8978900909423828, batch: 201 out of 1500\n",
      "loss: 1.952419638633728, batch: 251 out of 1500\n",
      "loss: 1.914036750793457, batch: 301 out of 1500\n",
      "loss: 1.8579750061035156, batch: 351 out of 1500\n",
      "loss: 1.9303796291351318, batch: 401 out of 1500\n",
      "loss: 1.896239995956421, batch: 451 out of 1500\n",
      "loss: 1.9401206970214844, batch: 501 out of 1500\n",
      "loss: 2.0151119232177734, batch: 551 out of 1500\n",
      "loss: 1.817518711090088, batch: 601 out of 1500\n",
      "loss: 1.8206477165222168, batch: 651 out of 1500\n",
      "loss: 1.9805697202682495, batch: 701 out of 1500\n",
      "loss: 1.794541358947754, batch: 751 out of 1500\n",
      "loss: 1.7917643785476685, batch: 801 out of 1500\n",
      "loss: 1.9558382034301758, batch: 851 out of 1500\n",
      "loss: 1.9941284656524658, batch: 901 out of 1500\n",
      "loss: 1.7000535726547241, batch: 951 out of 1500\n",
      "loss: 1.9251134395599365, batch: 1001 out of 1500\n",
      "loss: 1.890769600868225, batch: 1051 out of 1500\n",
      "loss: 1.8905621767044067, batch: 1101 out of 1500\n",
      "loss: 1.795966625213623, batch: 1151 out of 1500\n",
      "loss: 1.7634305953979492, batch: 1201 out of 1500\n",
      "loss: 1.9904264211654663, batch: 1251 out of 1500\n",
      "loss: 2.0174670219421387, batch: 1301 out of 1500\n",
      "loss: 1.846684455871582, batch: 1351 out of 1500\n",
      "loss: 1.9695789813995361, batch: 1401 out of 1500\n",
      "loss: 1.9084603786468506, batch: 1451 out of 1500\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.813760757446289, batch: 1 out of 1500\n",
      "loss: 1.9921178817749023, batch: 51 out of 1500\n",
      "loss: 1.804761290550232, batch: 101 out of 1500\n",
      "loss: 2.106719493865967, batch: 151 out of 1500\n",
      "loss: 1.7255494594573975, batch: 201 out of 1500\n",
      "loss: 1.6464768648147583, batch: 251 out of 1500\n",
      "loss: 1.878075122833252, batch: 301 out of 1500\n",
      "loss: 1.91806960105896, batch: 351 out of 1500\n",
      "loss: 1.9213557243347168, batch: 401 out of 1500\n",
      "loss: 1.9563708305358887, batch: 451 out of 1500\n",
      "loss: 1.9166126251220703, batch: 501 out of 1500\n",
      "loss: 1.8921386003494263, batch: 551 out of 1500\n",
      "loss: 1.8354544639587402, batch: 601 out of 1500\n",
      "loss: 2.0253074169158936, batch: 651 out of 1500\n",
      "loss: 1.898246169090271, batch: 701 out of 1500\n",
      "loss: 1.933171033859253, batch: 751 out of 1500\n",
      "loss: 1.8264098167419434, batch: 801 out of 1500\n",
      "loss: 1.9037481546401978, batch: 851 out of 1500\n",
      "loss: 1.8632864952087402, batch: 901 out of 1500\n",
      "loss: 1.908257246017456, batch: 951 out of 1500\n",
      "loss: 1.8763893842697144, batch: 1001 out of 1500\n",
      "loss: 1.96238112449646, batch: 1051 out of 1500\n",
      "loss: 1.9619847536087036, batch: 1101 out of 1500\n",
      "loss: 1.8428103923797607, batch: 1151 out of 1500\n",
      "loss: 1.771899700164795, batch: 1201 out of 1500\n",
      "loss: 1.7992621660232544, batch: 1251 out of 1500\n",
      "loss: 1.905695915222168, batch: 1301 out of 1500\n",
      "loss: 1.8728758096694946, batch: 1351 out of 1500\n",
      "loss: 1.9946277141571045, batch: 1401 out of 1500\n",
      "loss: 1.8887215852737427, batch: 1451 out of 1500\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 2.027909517288208, batch: 1 out of 1500\n",
      "loss: 1.6964646577835083, batch: 51 out of 1500\n",
      "loss: 1.9103318452835083, batch: 101 out of 1500\n",
      "loss: 2.0122060775756836, batch: 151 out of 1500\n",
      "loss: 1.934482216835022, batch: 201 out of 1500\n",
      "loss: 2.0733752250671387, batch: 251 out of 1500\n",
      "loss: 1.8795615434646606, batch: 301 out of 1500\n",
      "loss: 1.9810190200805664, batch: 351 out of 1500\n",
      "loss: 1.8986384868621826, batch: 401 out of 1500\n",
      "loss: 1.9306910037994385, batch: 451 out of 1500\n",
      "loss: 1.9240782260894775, batch: 501 out of 1500\n",
      "loss: 1.7596333026885986, batch: 551 out of 1500\n",
      "loss: 1.9354851245880127, batch: 601 out of 1500\n",
      "loss: 2.028815269470215, batch: 651 out of 1500\n",
      "loss: 1.8919212818145752, batch: 701 out of 1500\n",
      "loss: 1.8874151706695557, batch: 751 out of 1500\n",
      "loss: 1.7447190284729004, batch: 801 out of 1500\n",
      "loss: 1.9711122512817383, batch: 851 out of 1500\n",
      "loss: 1.6957881450653076, batch: 901 out of 1500\n",
      "loss: 1.8995888233184814, batch: 951 out of 1500\n",
      "loss: 1.9255826473236084, batch: 1001 out of 1500\n",
      "loss: 1.844918966293335, batch: 1051 out of 1500\n",
      "loss: 1.8781499862670898, batch: 1101 out of 1500\n",
      "loss: 2.000612258911133, batch: 1151 out of 1500\n",
      "loss: 1.9506518840789795, batch: 1201 out of 1500\n",
      "loss: 1.910495400428772, batch: 1251 out of 1500\n",
      "loss: 1.9607458114624023, batch: 1301 out of 1500\n",
      "loss: 1.949192762374878, batch: 1351 out of 1500\n",
      "loss: 1.8898720741271973, batch: 1401 out of 1500\n",
      "loss: 1.7548401355743408, batch: 1451 out of 1500\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.9352025985717773, batch: 1 out of 1500\n",
      "loss: 1.8718479871749878, batch: 51 out of 1500\n",
      "loss: 1.9727401733398438, batch: 101 out of 1500\n",
      "loss: 1.8833361864089966, batch: 151 out of 1500\n",
      "loss: 1.9006249904632568, batch: 201 out of 1500\n",
      "loss: 1.9067524671554565, batch: 251 out of 1500\n",
      "loss: 1.7518048286437988, batch: 301 out of 1500\n",
      "loss: 1.8138315677642822, batch: 351 out of 1500\n",
      "loss: 1.8866209983825684, batch: 401 out of 1500\n",
      "loss: 2.034984827041626, batch: 451 out of 1500\n",
      "loss: 1.8550246953964233, batch: 501 out of 1500\n",
      "loss: 1.758281946182251, batch: 551 out of 1500\n",
      "loss: 1.8843352794647217, batch: 601 out of 1500\n",
      "loss: 1.9368236064910889, batch: 651 out of 1500\n",
      "loss: 1.867659091949463, batch: 701 out of 1500\n",
      "loss: 1.8873753547668457, batch: 751 out of 1500\n",
      "loss: 1.7261791229248047, batch: 801 out of 1500\n",
      "loss: 1.9121403694152832, batch: 851 out of 1500\n",
      "loss: 1.7409709692001343, batch: 901 out of 1500\n",
      "loss: 1.9391870498657227, batch: 951 out of 1500\n",
      "loss: 1.8180344104766846, batch: 1001 out of 1500\n",
      "loss: 2.0887999534606934, batch: 1051 out of 1500\n",
      "loss: 1.8399322032928467, batch: 1101 out of 1500\n",
      "loss: 1.9242966175079346, batch: 1151 out of 1500\n",
      "loss: 1.8673796653747559, batch: 1201 out of 1500\n",
      "loss: 1.9102163314819336, batch: 1251 out of 1500\n",
      "loss: 1.843653678894043, batch: 1301 out of 1500\n",
      "loss: 1.6819343566894531, batch: 1351 out of 1500\n",
      "loss: 1.9847097396850586, batch: 1401 out of 1500\n",
      "loss: 1.8065612316131592, batch: 1451 out of 1500\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.9486324787139893, batch: 1 out of 1500\n",
      "loss: 1.9305224418640137, batch: 51 out of 1500\n",
      "loss: 1.8199745416641235, batch: 101 out of 1500\n",
      "loss: 1.8559026718139648, batch: 151 out of 1500\n",
      "loss: 1.8017609119415283, batch: 201 out of 1500\n",
      "loss: 1.909433126449585, batch: 251 out of 1500\n",
      "loss: 1.8469130992889404, batch: 301 out of 1500\n",
      "loss: 2.0164036750793457, batch: 351 out of 1500\n",
      "loss: 1.937695860862732, batch: 401 out of 1500\n",
      "loss: 1.7328684329986572, batch: 451 out of 1500\n",
      "loss: 1.800701379776001, batch: 501 out of 1500\n",
      "loss: 1.8453021049499512, batch: 551 out of 1500\n",
      "loss: 1.9249602556228638, batch: 601 out of 1500\n",
      "loss: 2.0241425037384033, batch: 651 out of 1500\n",
      "loss: 1.8166358470916748, batch: 701 out of 1500\n",
      "loss: 1.91740083694458, batch: 751 out of 1500\n",
      "loss: 1.9102797508239746, batch: 801 out of 1500\n",
      "loss: 1.8656648397445679, batch: 851 out of 1500\n",
      "loss: 1.7712023258209229, batch: 901 out of 1500\n",
      "loss: 1.8628425598144531, batch: 951 out of 1500\n",
      "loss: 1.7510786056518555, batch: 1001 out of 1500\n",
      "loss: 1.9904266595840454, batch: 1051 out of 1500\n",
      "loss: 1.9284504652023315, batch: 1101 out of 1500\n",
      "loss: 1.7675068378448486, batch: 1151 out of 1500\n",
      "loss: 1.7821075916290283, batch: 1201 out of 1500\n",
      "loss: 1.9108586311340332, batch: 1251 out of 1500\n",
      "loss: 1.9813684225082397, batch: 1301 out of 1500\n",
      "loss: 1.776887059211731, batch: 1351 out of 1500\n",
      "loss: 1.9028609991073608, batch: 1401 out of 1500\n",
      "loss: 1.890153408050537, batch: 1451 out of 1500\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.8883858919143677, batch: 1 out of 1500\n",
      "loss: 1.8445683717727661, batch: 51 out of 1500\n",
      "loss: 1.9718948602676392, batch: 101 out of 1500\n",
      "loss: 1.8707821369171143, batch: 151 out of 1500\n",
      "loss: 1.8771867752075195, batch: 201 out of 1500\n",
      "loss: 1.9646672010421753, batch: 251 out of 1500\n",
      "loss: 1.8726861476898193, batch: 301 out of 1500\n",
      "loss: 1.7631447315216064, batch: 351 out of 1500\n",
      "loss: 1.8935269117355347, batch: 401 out of 1500\n",
      "loss: 1.8717496395111084, batch: 451 out of 1500\n",
      "loss: 1.9996598958969116, batch: 501 out of 1500\n",
      "loss: 1.7732876539230347, batch: 551 out of 1500\n",
      "loss: 1.7938790321350098, batch: 601 out of 1500\n",
      "loss: 1.9797632694244385, batch: 651 out of 1500\n",
      "loss: 1.8990341424942017, batch: 701 out of 1500\n",
      "loss: 1.8957788944244385, batch: 751 out of 1500\n",
      "loss: 1.9121112823486328, batch: 801 out of 1500\n",
      "loss: 2.003721237182617, batch: 851 out of 1500\n",
      "loss: 1.918135404586792, batch: 901 out of 1500\n",
      "loss: 1.9168039560317993, batch: 951 out of 1500\n",
      "loss: 1.9004861116409302, batch: 1001 out of 1500\n",
      "loss: 1.8515021800994873, batch: 1051 out of 1500\n",
      "loss: 1.9047353267669678, batch: 1101 out of 1500\n",
      "loss: 2.0387697219848633, batch: 1151 out of 1500\n",
      "loss: 1.8977327346801758, batch: 1201 out of 1500\n",
      "loss: 1.9660613536834717, batch: 1251 out of 1500\n",
      "loss: 1.8687517642974854, batch: 1301 out of 1500\n",
      "loss: 1.7602720260620117, batch: 1351 out of 1500\n",
      "loss: 1.921807885169983, batch: 1401 out of 1500\n",
      "loss: 1.8953516483306885, batch: 1451 out of 1500\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.7579550743103027, batch: 1 out of 1500\n",
      "loss: 1.9816458225250244, batch: 51 out of 1500\n",
      "loss: 1.965055227279663, batch: 101 out of 1500\n",
      "loss: 1.6791108846664429, batch: 151 out of 1500\n",
      "loss: 1.8908114433288574, batch: 201 out of 1500\n",
      "loss: 1.8932079076766968, batch: 251 out of 1500\n",
      "loss: 1.8802584409713745, batch: 301 out of 1500\n",
      "loss: 1.7526023387908936, batch: 351 out of 1500\n",
      "loss: 1.7226784229278564, batch: 401 out of 1500\n",
      "loss: 1.9147942066192627, batch: 451 out of 1500\n",
      "loss: 1.8393802642822266, batch: 501 out of 1500\n",
      "loss: 1.8499858379364014, batch: 551 out of 1500\n",
      "loss: 1.9118191003799438, batch: 601 out of 1500\n",
      "loss: 1.8135581016540527, batch: 651 out of 1500\n",
      "loss: 1.8426127433776855, batch: 701 out of 1500\n",
      "loss: 2.0540547370910645, batch: 751 out of 1500\n",
      "loss: 1.8423511981964111, batch: 801 out of 1500\n",
      "loss: 1.9041879177093506, batch: 851 out of 1500\n",
      "loss: 1.875035285949707, batch: 901 out of 1500\n",
      "loss: 1.9890846014022827, batch: 951 out of 1500\n",
      "loss: 1.905783772468567, batch: 1001 out of 1500\n",
      "loss: 1.752212643623352, batch: 1051 out of 1500\n",
      "loss: 1.8037700653076172, batch: 1101 out of 1500\n",
      "loss: 1.8673591613769531, batch: 1151 out of 1500\n",
      "loss: 1.7949328422546387, batch: 1201 out of 1500\n",
      "loss: 1.9248290061950684, batch: 1251 out of 1500\n",
      "loss: 1.7982206344604492, batch: 1301 out of 1500\n",
      "loss: 1.9322423934936523, batch: 1351 out of 1500\n",
      "loss: 1.8952720165252686, batch: 1401 out of 1500\n",
      "loss: 1.7641487121582031, batch: 1451 out of 1500\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.9061720371246338, batch: 1 out of 1500\n",
      "loss: 1.8812249898910522, batch: 51 out of 1500\n",
      "loss: 1.8189064264297485, batch: 101 out of 1500\n",
      "loss: 1.765866994857788, batch: 151 out of 1500\n",
      "loss: 1.790748119354248, batch: 201 out of 1500\n",
      "loss: 1.91132390499115, batch: 251 out of 1500\n",
      "loss: 1.8444551229476929, batch: 301 out of 1500\n",
      "loss: 1.9812268018722534, batch: 351 out of 1500\n",
      "loss: 1.7583746910095215, batch: 401 out of 1500\n",
      "loss: 1.8717916011810303, batch: 451 out of 1500\n",
      "loss: 1.9862231016159058, batch: 501 out of 1500\n",
      "loss: 1.8919906616210938, batch: 551 out of 1500\n",
      "loss: 1.7566550970077515, batch: 601 out of 1500\n",
      "loss: 1.8349192142486572, batch: 651 out of 1500\n",
      "loss: 1.9494036436080933, batch: 701 out of 1500\n",
      "loss: 1.6737151145935059, batch: 751 out of 1500\n",
      "loss: 2.0028679370880127, batch: 801 out of 1500\n",
      "loss: 1.9152123928070068, batch: 851 out of 1500\n",
      "loss: 1.8929808139801025, batch: 901 out of 1500\n",
      "loss: 1.869128704071045, batch: 951 out of 1500\n",
      "loss: 1.8182532787322998, batch: 1001 out of 1500\n",
      "loss: 1.968977928161621, batch: 1051 out of 1500\n",
      "loss: 1.8401654958724976, batch: 1101 out of 1500\n",
      "loss: 1.7989213466644287, batch: 1151 out of 1500\n",
      "loss: 1.8285642862319946, batch: 1201 out of 1500\n",
      "loss: 1.8547911643981934, batch: 1251 out of 1500\n",
      "loss: 1.8698949813842773, batch: 1301 out of 1500\n",
      "loss: 1.8054232597351074, batch: 1351 out of 1500\n",
      "loss: 1.8282278776168823, batch: 1401 out of 1500\n",
      "loss: 1.7879829406738281, batch: 1451 out of 1500\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.8058744668960571, batch: 1 out of 1500\n",
      "loss: 1.8024890422821045, batch: 51 out of 1500\n",
      "loss: 1.7277847528457642, batch: 101 out of 1500\n",
      "loss: 1.764244556427002, batch: 151 out of 1500\n",
      "loss: 1.8651113510131836, batch: 201 out of 1500\n",
      "loss: 2.036780595779419, batch: 251 out of 1500\n",
      "loss: 1.9961720705032349, batch: 301 out of 1500\n",
      "loss: 1.8477296829223633, batch: 351 out of 1500\n",
      "loss: 1.90205717086792, batch: 401 out of 1500\n",
      "loss: 1.7463197708129883, batch: 451 out of 1500\n",
      "loss: 1.9670690298080444, batch: 501 out of 1500\n",
      "loss: 1.867974042892456, batch: 551 out of 1500\n",
      "loss: 1.8408575057983398, batch: 601 out of 1500\n",
      "loss: 1.9562138319015503, batch: 651 out of 1500\n",
      "loss: 1.9623888731002808, batch: 701 out of 1500\n",
      "loss: 1.9130946397781372, batch: 751 out of 1500\n",
      "loss: 1.8564229011535645, batch: 801 out of 1500\n",
      "loss: 1.9801486730575562, batch: 851 out of 1500\n",
      "loss: 1.8479417562484741, batch: 901 out of 1500\n",
      "loss: 1.8377926349639893, batch: 951 out of 1500\n",
      "loss: 1.7560431957244873, batch: 1001 out of 1500\n",
      "loss: 1.9651129245758057, batch: 1051 out of 1500\n",
      "loss: 1.8405886888504028, batch: 1101 out of 1500\n",
      "loss: 1.854385256767273, batch: 1151 out of 1500\n",
      "loss: 1.7509231567382812, batch: 1201 out of 1500\n",
      "loss: 1.8114187717437744, batch: 1251 out of 1500\n",
      "loss: 1.7401704788208008, batch: 1301 out of 1500\n",
      "loss: 2.000810384750366, batch: 1351 out of 1500\n",
      "loss: 1.9160534143447876, batch: 1401 out of 1500\n",
      "loss: 1.9736241102218628, batch: 1451 out of 1500\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.905184030532837, batch: 1 out of 1500\n",
      "loss: 1.9855875968933105, batch: 51 out of 1500\n",
      "loss: 1.650369644165039, batch: 101 out of 1500\n",
      "loss: 1.7855665683746338, batch: 151 out of 1500\n",
      "loss: 1.9906368255615234, batch: 201 out of 1500\n",
      "loss: 1.842644453048706, batch: 251 out of 1500\n",
      "loss: 1.8466384410858154, batch: 301 out of 1500\n",
      "loss: 1.8483335971832275, batch: 351 out of 1500\n",
      "loss: 1.811450481414795, batch: 401 out of 1500\n",
      "loss: 1.775309681892395, batch: 451 out of 1500\n",
      "loss: 1.750251054763794, batch: 501 out of 1500\n",
      "loss: 1.9188919067382812, batch: 551 out of 1500\n",
      "loss: 1.9289121627807617, batch: 601 out of 1500\n",
      "loss: 1.994140625, batch: 651 out of 1500\n",
      "loss: 1.7798044681549072, batch: 701 out of 1500\n",
      "loss: 1.7739667892456055, batch: 751 out of 1500\n",
      "loss: 1.9303784370422363, batch: 801 out of 1500\n",
      "loss: 1.6993989944458008, batch: 851 out of 1500\n",
      "loss: 1.8684144020080566, batch: 901 out of 1500\n",
      "loss: 1.9569263458251953, batch: 951 out of 1500\n",
      "loss: 1.8736639022827148, batch: 1001 out of 1500\n",
      "loss: 1.9572407007217407, batch: 1051 out of 1500\n",
      "loss: 1.7185649871826172, batch: 1101 out of 1500\n",
      "loss: 1.8883254528045654, batch: 1151 out of 1500\n",
      "loss: 1.8812041282653809, batch: 1201 out of 1500\n",
      "loss: 1.980578899383545, batch: 1251 out of 1500\n",
      "loss: 2.083489179611206, batch: 1301 out of 1500\n",
      "loss: 1.8024811744689941, batch: 1351 out of 1500\n",
      "loss: 1.992967128753662, batch: 1401 out of 1500\n",
      "loss: 1.986363410949707, batch: 1451 out of 1500\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.9630948305130005, batch: 1 out of 1500\n",
      "loss: 1.9646117687225342, batch: 51 out of 1500\n",
      "loss: 1.974386215209961, batch: 101 out of 1500\n",
      "loss: 1.7824766635894775, batch: 151 out of 1500\n",
      "loss: 1.8273502588272095, batch: 201 out of 1500\n",
      "loss: 1.7746220827102661, batch: 251 out of 1500\n",
      "loss: 1.8875997066497803, batch: 301 out of 1500\n",
      "loss: 1.8828461170196533, batch: 351 out of 1500\n",
      "loss: 1.9301669597625732, batch: 401 out of 1500\n",
      "loss: 1.820220708847046, batch: 451 out of 1500\n",
      "loss: 1.8700428009033203, batch: 501 out of 1500\n",
      "loss: 1.9127273559570312, batch: 551 out of 1500\n",
      "loss: 1.8130913972854614, batch: 601 out of 1500\n",
      "loss: 1.7486941814422607, batch: 651 out of 1500\n",
      "loss: 2.0077147483825684, batch: 701 out of 1500\n",
      "loss: 1.825549602508545, batch: 751 out of 1500\n",
      "loss: 1.8929733037948608, batch: 801 out of 1500\n",
      "loss: 1.8918118476867676, batch: 851 out of 1500\n",
      "loss: 1.9049956798553467, batch: 901 out of 1500\n",
      "loss: 1.7780035734176636, batch: 951 out of 1500\n",
      "loss: 1.8979387283325195, batch: 1001 out of 1500\n",
      "loss: 1.8212156295776367, batch: 1051 out of 1500\n",
      "loss: 1.7766904830932617, batch: 1101 out of 1500\n",
      "loss: 1.9159071445465088, batch: 1151 out of 1500\n",
      "loss: 1.8978129625320435, batch: 1201 out of 1500\n",
      "loss: 1.8876235485076904, batch: 1251 out of 1500\n",
      "loss: 1.8661656379699707, batch: 1301 out of 1500\n",
      "loss: 1.7586932182312012, batch: 1351 out of 1500\n",
      "loss: 1.8859535455703735, batch: 1401 out of 1500\n",
      "loss: 1.7613465785980225, batch: 1451 out of 1500\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.8040261268615723, batch: 1 out of 1500\n",
      "loss: 1.8244144916534424, batch: 51 out of 1500\n",
      "loss: 1.8643449544906616, batch: 101 out of 1500\n",
      "loss: 1.817847728729248, batch: 151 out of 1500\n",
      "loss: 1.7180507183074951, batch: 201 out of 1500\n",
      "loss: 1.934565544128418, batch: 251 out of 1500\n",
      "loss: 1.8083057403564453, batch: 301 out of 1500\n",
      "loss: 1.9361382722854614, batch: 351 out of 1500\n",
      "loss: 1.944467306137085, batch: 401 out of 1500\n",
      "loss: 1.9301631450653076, batch: 451 out of 1500\n",
      "loss: 1.7480965852737427, batch: 501 out of 1500\n",
      "loss: 1.823551893234253, batch: 551 out of 1500\n",
      "loss: 1.7506948709487915, batch: 601 out of 1500\n",
      "loss: 1.7570695877075195, batch: 651 out of 1500\n",
      "loss: 1.8850274085998535, batch: 701 out of 1500\n",
      "loss: 1.7432782649993896, batch: 751 out of 1500\n",
      "loss: 1.851603627204895, batch: 801 out of 1500\n",
      "loss: 1.8314714431762695, batch: 851 out of 1500\n",
      "loss: 1.8917787075042725, batch: 901 out of 1500\n",
      "loss: 1.9303317070007324, batch: 951 out of 1500\n",
      "loss: 1.7292485237121582, batch: 1001 out of 1500\n",
      "loss: 1.8653658628463745, batch: 1051 out of 1500\n",
      "loss: 1.8854669332504272, batch: 1101 out of 1500\n",
      "loss: 1.8772215843200684, batch: 1151 out of 1500\n",
      "loss: 1.8762097358703613, batch: 1201 out of 1500\n",
      "loss: 1.9172483682632446, batch: 1251 out of 1500\n",
      "loss: 2.033987283706665, batch: 1301 out of 1500\n",
      "loss: 1.8997538089752197, batch: 1351 out of 1500\n",
      "loss: 1.9849902391433716, batch: 1401 out of 1500\n",
      "loss: 1.8624382019042969, batch: 1451 out of 1500\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.8067448139190674, batch: 1 out of 1500\n",
      "loss: 1.9884235858917236, batch: 51 out of 1500\n",
      "loss: 1.676515817642212, batch: 101 out of 1500\n",
      "loss: 1.8887393474578857, batch: 151 out of 1500\n",
      "loss: 1.9104578495025635, batch: 201 out of 1500\n",
      "loss: 1.9935158491134644, batch: 251 out of 1500\n",
      "loss: 1.8788422346115112, batch: 301 out of 1500\n",
      "loss: 1.8501708507537842, batch: 351 out of 1500\n",
      "loss: 1.8422869443893433, batch: 401 out of 1500\n",
      "loss: 1.8352421522140503, batch: 451 out of 1500\n",
      "loss: 1.8261444568634033, batch: 501 out of 1500\n",
      "loss: 1.8621447086334229, batch: 551 out of 1500\n",
      "loss: 1.7915146350860596, batch: 601 out of 1500\n",
      "loss: 1.946496605873108, batch: 651 out of 1500\n",
      "loss: 1.8984192609786987, batch: 701 out of 1500\n",
      "loss: 1.8759766817092896, batch: 751 out of 1500\n",
      "loss: 1.8839013576507568, batch: 801 out of 1500\n",
      "loss: 1.911374568939209, batch: 851 out of 1500\n",
      "loss: 1.8490713834762573, batch: 901 out of 1500\n",
      "loss: 1.8185038566589355, batch: 951 out of 1500\n",
      "loss: 1.8668791055679321, batch: 1001 out of 1500\n",
      "loss: 1.9951547384262085, batch: 1051 out of 1500\n",
      "loss: 1.8436000347137451, batch: 1101 out of 1500\n",
      "loss: 1.8206477165222168, batch: 1151 out of 1500\n",
      "loss: 1.8735742568969727, batch: 1201 out of 1500\n",
      "loss: 1.8623342514038086, batch: 1251 out of 1500\n",
      "loss: 1.7895828485488892, batch: 1301 out of 1500\n",
      "loss: 1.8835535049438477, batch: 1351 out of 1500\n",
      "loss: 1.6797266006469727, batch: 1401 out of 1500\n",
      "loss: 1.8576289415359497, batch: 1451 out of 1500\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.8662755489349365, batch: 1 out of 1500\n",
      "loss: 1.7131891250610352, batch: 51 out of 1500\n",
      "loss: 1.9405221939086914, batch: 101 out of 1500\n",
      "loss: 1.8000203371047974, batch: 151 out of 1500\n",
      "loss: 1.9542458057403564, batch: 201 out of 1500\n",
      "loss: 1.8635767698287964, batch: 251 out of 1500\n",
      "loss: 1.8198578357696533, batch: 301 out of 1500\n",
      "loss: 1.8486566543579102, batch: 351 out of 1500\n",
      "loss: 1.8675878047943115, batch: 401 out of 1500\n",
      "loss: 1.9428783655166626, batch: 451 out of 1500\n",
      "loss: 1.8834600448608398, batch: 501 out of 1500\n",
      "loss: 1.814074993133545, batch: 551 out of 1500\n",
      "loss: 1.7895758152008057, batch: 601 out of 1500\n",
      "loss: 1.820865511894226, batch: 651 out of 1500\n",
      "loss: 1.8239492177963257, batch: 701 out of 1500\n",
      "loss: 1.7878797054290771, batch: 751 out of 1500\n",
      "loss: 2.0553393363952637, batch: 801 out of 1500\n",
      "loss: 1.760742425918579, batch: 851 out of 1500\n",
      "loss: 1.8381630182266235, batch: 901 out of 1500\n",
      "loss: 1.7319201231002808, batch: 951 out of 1500\n",
      "loss: 1.8469994068145752, batch: 1001 out of 1500\n",
      "loss: 1.9306178092956543, batch: 1051 out of 1500\n",
      "loss: 2.0052437782287598, batch: 1101 out of 1500\n",
      "loss: 1.832714319229126, batch: 1151 out of 1500\n",
      "loss: 1.8921555280685425, batch: 1201 out of 1500\n",
      "loss: 1.8213694095611572, batch: 1251 out of 1500\n",
      "loss: 1.7749509811401367, batch: 1301 out of 1500\n",
      "loss: 1.8261075019836426, batch: 1351 out of 1500\n",
      "loss: 1.8261239528656006, batch: 1401 out of 1500\n",
      "loss: 1.9163485765457153, batch: 1451 out of 1500\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 1.858202 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "perf_timer = time.perf_counter()\n",
    "perf_acc = \"\"\n",
    "\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_function, optimizer)\n",
    "    perf_acc = test(test_dataloader, model, loss_function)\n",
    "    \n",
    "print(perf_acc)\n",
    "perf_timer = time.perf_counter() - perf_timer\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"cnn_11\"\n",
    "if True:\n",
    "    torch.save(model.state_dict(), MODELS_PATH + model_name + \".pth\")\n",
    "\n",
    "    with open(MODELS_PATH + model_name + \".txt\", \"w\") as f:\n",
    "        f.write(\"Epochs: {}\\n\".format(EPOCHS))\n",
    "        f.write(\"Batch Size: {}\\n\".format(BATCH_SIZE))\n",
    "        f.write(\"Sample Size: {}\\n\".format(SAMPLE_SIZE))\n",
    "        f.write(\"Feature Set: {}\\n\".format(FEATURES))\n",
    "        f.write(\"Model: {}\\n\".format(str(model)))\n",
    "        f.write(\"Loss Function: {}\\n\".format(\"Cross Entropy Loss\"))\n",
    "        f.write(\"Optimizer: {}\\n\\n\\n\\n\".format(str(optimizer)))\n",
    "        f.write(\"Results: {}\\n\".format(perf_acc))\n",
    "        f.write(\"Timer: {}s\\n\".format(round(perf_timer, 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
