{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "\n",
    "import torch\n",
    "import mrmr\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_PATH = './data/'\n",
    "DATASET_PATH = DATA_PATH + 'smartphone+based+recognition+of+human+activities+and+postural+transitions/'\n",
    "MODELS_PATH = DATA_PATH + 'models/raw-models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_set = pd.read_csv(DATA_PATH + \"self-calculated/raw-data/train-data.txt\", sep='\\s+')\n",
    "test_set = pd.read_csv(DATA_PATH + \"self-calculated/raw-data/test-data.txt\", sep='\\s+')\n",
    "\n",
    "train_set.columns=['accX', 'accY', 'accZ', 'gyroX', 'gyroY', 'gyroZ', \"subject\", \"activity\"]\n",
    "test_set.columns=['accX', 'accY', 'accZ', 'gyroX', 'gyroY', 'gyroZ', \"subject\", \"activity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model\n",
    "device = None\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "class RawDataModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RawDataModel, self).__init__()\n",
    "        self.num_layers = 2\n",
    "        self.hidden_size = 16\n",
    "\n",
    "        self.lstm = nn.LSTM(7, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.l1 = nn.Linear(self.hidden_size, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(0) \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.l1(out[:, -1, :])\n",
    "\n",
    "        if batch_size == 1:\n",
    "            out = out.squeeze(0)\n",
    "\n",
    "        return out\n",
    "    \n",
    "model = RawDataModel().to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing function\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        y = y.squeeze()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            y = y.squeeze()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    return(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset preparation\n",
    "class HAPTDataset(Dataset):\n",
    "    def __init__(self, dataset, features, label):\n",
    "        self.data = torch.tensor(dataset[features].values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(dataset[label].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "    \n",
    "train_dataset = HAPTDataset(train_set, train_set.columns[:-1], 'activity')\n",
    "test_dataset = HAPTDataset(test_set, test_set.columns[:-1], 'activity')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 61.318604  [    8/570908]\n",
      "loss: 64.888290  [  808/570908]\n",
      "loss: 92.366600  [ 1608/570908]\n",
      "loss: 61.253162  [ 2408/570908]\n",
      "loss: 70.748306  [ 3208/570908]\n",
      "loss: 89.588898  [ 4008/570908]\n",
      "loss: 54.575150  [ 4808/570908]\n",
      "loss: 86.939819  [ 5608/570908]\n",
      "loss: 44.195137  [ 6408/570908]\n",
      "loss: 66.636627  [ 7208/570908]\n",
      "loss: 66.368637  [ 8008/570908]\n",
      "loss: 88.700851  [ 8808/570908]\n",
      "loss: 41.451111  [ 9608/570908]\n",
      "loss: 68.903122  [10408/570908]\n",
      "loss: 72.621750  [11208/570908]\n",
      "loss: 74.934059  [12008/570908]\n",
      "loss: 74.834961  [12808/570908]\n",
      "loss: 51.881752  [13608/570908]\n",
      "loss: 68.437286  [14408/570908]\n",
      "loss: 68.453934  [15208/570908]\n",
      "loss: 64.389618  [16008/570908]\n",
      "loss: 68.616974  [16808/570908]\n",
      "loss: 41.543396  [17608/570908]\n",
      "loss: 68.805038  [18408/570908]\n",
      "loss: 87.349182  [19208/570908]\n",
      "loss: 83.233475  [20008/570908]\n",
      "loss: 85.187019  [20808/570908]\n",
      "loss: 62.411819  [21608/570908]\n",
      "loss: 70.628769  [22408/570908]\n",
      "loss: 62.495502  [23208/570908]\n",
      "loss: 74.805023  [24008/570908]\n",
      "loss: 72.698372  [24808/570908]\n",
      "loss: 45.720383  [25608/570908]\n",
      "loss: 60.357262  [26408/570908]\n",
      "loss: 79.024933  [27208/570908]\n",
      "loss: 85.258484  [28008/570908]\n",
      "loss: 76.831207  [28808/570908]\n",
      "loss: 64.615273  [29608/570908]\n",
      "loss: 66.336853  [30408/570908]\n",
      "loss: 52.047028  [31208/570908]\n",
      "loss: 62.475620  [32008/570908]\n",
      "loss: 79.608551  [32808/570908]\n",
      "loss: 64.798676  [33608/570908]\n",
      "loss: 66.402046  [34408/570908]\n",
      "loss: 81.006729  [35208/570908]\n",
      "loss: 60.381447  [36008/570908]\n",
      "loss: 74.980164  [36808/570908]\n",
      "loss: 45.627918  [37608/570908]\n",
      "loss: 83.042793  [38408/570908]\n",
      "loss: 66.551567  [39208/570908]\n",
      "loss: 45.851578  [40008/570908]\n",
      "loss: 72.757126  [40808/570908]\n",
      "loss: 81.110336  [41608/570908]\n",
      "loss: 91.619598  [42408/570908]\n",
      "loss: 54.084160  [43208/570908]\n",
      "loss: 49.901554  [44008/570908]\n",
      "loss: 62.187374  [44808/570908]\n",
      "loss: 79.004181  [45608/570908]\n",
      "loss: 56.267090  [46408/570908]\n",
      "loss: 102.061470  [47208/570908]\n",
      "loss: 74.723572  [48008/570908]\n",
      "loss: 66.526741  [48808/570908]\n",
      "loss: 68.821892  [49608/570908]\n",
      "loss: 93.563217  [50408/570908]\n",
      "loss: 85.134094  [51208/570908]\n",
      "loss: 60.121517  [52008/570908]\n",
      "loss: 78.680725  [52808/570908]\n",
      "loss: 97.767349  [53608/570908]\n",
      "loss: 58.260292  [54408/570908]\n",
      "loss: 70.724915  [55208/570908]\n",
      "loss: 74.656410  [56008/570908]\n",
      "loss: 39.423191  [56808/570908]\n",
      "loss: 70.732162  [57608/570908]\n",
      "loss: 70.629547  [58408/570908]\n",
      "loss: 97.644989  [59208/570908]\n",
      "loss: 64.426231  [60008/570908]\n",
      "loss: 58.215721  [60808/570908]\n",
      "loss: 81.045822  [61608/570908]\n",
      "loss: 91.498962  [62408/570908]\n",
      "loss: 62.268303  [63208/570908]\n",
      "loss: 97.797989  [64008/570908]\n",
      "loss: 81.063126  [64808/570908]\n",
      "loss: 72.769226  [65608/570908]\n",
      "loss: 51.842171  [66408/570908]\n",
      "loss: 54.066986  [67208/570908]\n",
      "loss: 68.711746  [68008/570908]\n",
      "loss: 64.513855  [68808/570908]\n",
      "loss: 58.252785  [69608/570908]\n",
      "loss: 70.421219  [70408/570908]\n",
      "loss: 57.862450  [71208/570908]\n",
      "loss: 77.023178  [72008/570908]\n",
      "loss: 69.063484  [72808/570908]\n",
      "loss: 66.401253  [73608/570908]\n",
      "loss: 58.075527  [74408/570908]\n",
      "loss: 62.241730  [75208/570908]\n",
      "loss: 72.812378  [76008/570908]\n",
      "loss: 87.441208  [76808/570908]\n",
      "loss: 81.123520  [77608/570908]\n",
      "loss: 54.218330  [78408/570908]\n",
      "loss: 74.979897  [79208/570908]\n",
      "loss: 84.902145  [80008/570908]\n",
      "loss: 70.844208  [80808/570908]\n",
      "loss: 62.263954  [81608/570908]\n",
      "loss: 79.132431  [82408/570908]\n",
      "loss: 56.208046  [83208/570908]\n",
      "loss: 72.704849  [84008/570908]\n",
      "loss: 68.686401  [84808/570908]\n",
      "loss: 62.264664  [85608/570908]\n",
      "loss: 64.521881  [86408/570908]\n",
      "loss: 64.445404  [87208/570908]\n",
      "loss: 66.644318  [88008/570908]\n",
      "loss: 87.398911  [88808/570908]\n",
      "loss: 47.741909  [89608/570908]\n",
      "loss: 62.564064  [90408/570908]\n",
      "loss: 43.713985  [91208/570908]\n",
      "loss: 62.361027  [92008/570908]\n",
      "loss: 68.644363  [92808/570908]\n",
      "loss: 74.603081  [93608/570908]\n",
      "loss: 72.698387  [94408/570908]\n",
      "loss: 60.340057  [95208/570908]\n",
      "loss: 85.200027  [96008/570908]\n",
      "loss: 56.126343  [96808/570908]\n",
      "loss: 64.599075  [97608/570908]\n",
      "loss: 85.149719  [98408/570908]\n",
      "loss: 77.084579  [99208/570908]\n",
      "loss: 72.935272  [100008/570908]\n",
      "loss: 70.666290  [100808/570908]\n",
      "loss: 81.129761  [101608/570908]\n",
      "loss: 68.553497  [102408/570908]\n",
      "loss: 56.114189  [103208/570908]\n",
      "loss: 53.994408  [104008/570908]\n",
      "loss: 56.157066  [104808/570908]\n",
      "loss: 54.085686  [105608/570908]\n",
      "loss: 80.956284  [106408/570908]\n",
      "loss: 83.249886  [107208/570908]\n",
      "loss: 53.900223  [108008/570908]\n",
      "loss: 70.615402  [108808/570908]\n",
      "loss: 81.083450  [109608/570908]\n",
      "loss: 62.382347  [110408/570908]\n",
      "loss: 53.818176  [111208/570908]\n",
      "loss: 74.841469  [112008/570908]\n",
      "loss: 68.650116  [112808/570908]\n",
      "loss: 64.717514  [113608/570908]\n",
      "loss: 74.864990  [114408/570908]\n",
      "loss: 51.891853  [115208/570908]\n",
      "loss: 87.376312  [116008/570908]\n",
      "loss: 77.033081  [116808/570908]\n",
      "loss: 78.862106  [117608/570908]\n",
      "loss: 99.596130  [118408/570908]\n",
      "loss: 72.515549  [119208/570908]\n",
      "loss: 72.624725  [120008/570908]\n",
      "loss: 72.505508  [120808/570908]\n",
      "loss: 89.494507  [121608/570908]\n",
      "loss: 74.691330  [122408/570908]\n",
      "loss: 87.330353  [123208/570908]\n",
      "loss: 62.369408  [124008/570908]\n",
      "loss: 62.571541  [124808/570908]\n",
      "loss: 68.562332  [125608/570908]\n",
      "loss: 83.155312  [126408/570908]\n",
      "loss: 79.194946  [127208/570908]\n",
      "loss: 93.531540  [128008/570908]\n",
      "loss: 54.048782  [128808/570908]\n",
      "loss: 81.345230  [129608/570908]\n",
      "loss: 91.298836  [130408/570908]\n",
      "loss: 76.878960  [131208/570908]\n",
      "loss: 76.623413  [132008/570908]\n",
      "loss: 45.837334  [132808/570908]\n",
      "loss: 76.921326  [133608/570908]\n",
      "loss: 77.030060  [134408/570908]\n",
      "loss: 62.404037  [135208/570908]\n",
      "loss: 60.223881  [136008/570908]\n",
      "loss: 74.592484  [136808/570908]\n",
      "loss: 62.243271  [137608/570908]\n",
      "loss: 66.475212  [138408/570908]\n",
      "loss: 70.923874  [139208/570908]\n",
      "loss: 64.542358  [140008/570908]\n",
      "loss: 49.819733  [140808/570908]\n",
      "loss: 43.762985  [141608/570908]\n",
      "loss: 64.550171  [142408/570908]\n",
      "loss: 72.594833  [143208/570908]\n",
      "loss: 58.038635  [144008/570908]\n",
      "loss: 77.072128  [144808/570908]\n",
      "loss: 87.274933  [145608/570908]\n",
      "loss: 62.440300  [146408/570908]\n",
      "loss: 56.096844  [147208/570908]\n",
      "loss: 74.854996  [148008/570908]\n",
      "loss: 51.744347  [148808/570908]\n",
      "loss: 84.683731  [149608/570908]\n",
      "loss: 66.471024  [150408/570908]\n",
      "loss: 55.913712  [151208/570908]\n",
      "loss: 74.847160  [152008/570908]\n",
      "loss: 52.057625  [152808/570908]\n",
      "loss: 83.071289  [153608/570908]\n",
      "loss: 74.432617  [154408/570908]\n",
      "loss: 70.491089  [155208/570908]\n",
      "loss: 66.102402  [156008/570908]\n",
      "loss: 82.824310  [156808/570908]\n",
      "loss: 63.953041  [157608/570908]\n",
      "loss: 83.043953  [158408/570908]\n",
      "loss: 39.458076  [159208/570908]\n",
      "loss: 62.375916  [160008/570908]\n",
      "loss: 70.739510  [160808/570908]\n",
      "loss: 60.204132  [161608/570908]\n",
      "loss: 66.529221  [162408/570908]\n",
      "loss: 66.310768  [163208/570908]\n",
      "loss: 74.679306  [164008/570908]\n",
      "loss: 79.026215  [164808/570908]\n",
      "loss: 58.371098  [165608/570908]\n",
      "loss: 50.050270  [166408/570908]\n",
      "loss: 54.270264  [167208/570908]\n",
      "loss: 62.035225  [168008/570908]\n",
      "loss: 43.766773  [168808/570908]\n",
      "loss: 70.816483  [169608/570908]\n",
      "loss: 58.269562  [170408/570908]\n",
      "loss: 105.894424  [171208/570908]\n",
      "loss: 57.927818  [172008/570908]\n",
      "loss: 107.891167  [172808/570908]\n",
      "loss: 85.603333  [173608/570908]\n",
      "loss: 89.488953  [174408/570908]\n",
      "loss: 78.733734  [175208/570908]\n",
      "loss: 68.382919  [176008/570908]\n",
      "loss: 52.000420  [176808/570908]\n",
      "loss: 95.111549  [177608/570908]\n",
      "loss: 59.964333  [178408/570908]\n",
      "loss: 109.703850  [179208/570908]\n",
      "loss: 93.171844  [180008/570908]\n",
      "loss: 64.199440  [180808/570908]\n",
      "loss: 64.470245  [181608/570908]\n",
      "loss: 54.502914  [182408/570908]\n",
      "loss: 99.531387  [183208/570908]\n",
      "loss: 56.003204  [184008/570908]\n",
      "loss: 71.551804  [184808/570908]\n",
      "loss: 74.902634  [185608/570908]\n",
      "loss: 62.154213  [186408/570908]\n",
      "loss: 58.103592  [187208/570908]\n",
      "loss: 54.148693  [188008/570908]\n",
      "loss: 62.613819  [188808/570908]\n",
      "loss: 68.325279  [189608/570908]\n",
      "loss: 61.143471  [190408/570908]\n",
      "loss: 68.606842  [191208/570908]\n",
      "loss: 61.416763  [192008/570908]\n",
      "loss: 51.972748  [192808/570908]\n",
      "loss: 44.829014  [193608/570908]\n",
      "loss: 66.414909  [194408/570908]\n",
      "loss: 59.971268  [195208/570908]\n",
      "loss: 80.863922  [196008/570908]\n",
      "loss: 76.736969  [196808/570908]\n",
      "loss: 88.605148  [197608/570908]\n",
      "loss: 72.832054  [198408/570908]\n",
      "loss: 50.965054  [199208/570908]\n",
      "loss: 85.013069  [200008/570908]\n",
      "loss: 73.311729  [200808/570908]\n",
      "loss: 53.508804  [201608/570908]\n",
      "loss: 72.665161  [202408/570908]\n",
      "loss: 66.071144  [203208/570908]\n",
      "loss: 68.388885  [204008/570908]\n",
      "loss: 80.230843  [204808/570908]\n",
      "loss: 76.966156  [205608/570908]\n",
      "loss: 45.491699  [206408/570908]\n",
      "loss: 51.337589  [207208/570908]\n",
      "loss: 64.190781  [208008/570908]\n",
      "loss: 81.073799  [208808/570908]\n",
      "loss: 71.330223  [209608/570908]\n",
      "loss: 64.353386  [210408/570908]\n",
      "loss: 62.573414  [211208/570908]\n",
      "loss: 86.326340  [212008/570908]\n",
      "loss: 48.894928  [212808/570908]\n",
      "loss: 69.880142  [213608/570908]\n",
      "loss: 66.561852  [214408/570908]\n",
      "loss: 45.590252  [215208/570908]\n",
      "loss: 70.696327  [216008/570908]\n",
      "loss: 72.917267  [216808/570908]\n",
      "loss: 52.255074  [217608/570908]\n",
      "loss: 68.724152  [218408/570908]\n",
      "loss: 75.036652  [219208/570908]\n",
      "loss: 106.546600  [220008/570908]\n",
      "loss: 60.497177  [220808/570908]\n",
      "loss: 61.878860  [221608/570908]\n",
      "loss: 69.751793  [222408/570908]\n",
      "loss: 95.226212  [223208/570908]\n",
      "loss: 77.592560  [224008/570908]\n",
      "loss: 45.510330  [224808/570908]\n",
      "loss: 70.533714  [225608/570908]\n",
      "loss: 76.984276  [226408/570908]\n",
      "loss: 51.951855  [227208/570908]\n",
      "loss: 71.207520  [228008/570908]\n",
      "loss: 87.264023  [228808/570908]\n",
      "loss: 85.905029  [229608/570908]\n",
      "loss: 80.736740  [230408/570908]\n",
      "loss: 77.650505  [231208/570908]\n",
      "loss: 55.900459  [232008/570908]\n",
      "loss: 50.903530  [232808/570908]\n",
      "loss: 82.378334  [233608/570908]\n",
      "loss: 83.330620  [234408/570908]\n",
      "loss: 68.507843  [235208/570908]\n",
      "loss: 56.096737  [236008/570908]\n",
      "loss: 70.365036  [236808/570908]\n",
      "loss: 63.905746  [237608/570908]\n",
      "loss: 90.247498  [238408/570908]\n",
      "loss: 83.179337  [239208/570908]\n",
      "loss: 70.803253  [240008/570908]\n",
      "loss: 65.673431  [240808/570908]\n",
      "loss: 59.714497  [241608/570908]\n",
      "loss: 43.712074  [242408/570908]\n",
      "loss: 63.191566  [243208/570908]\n",
      "loss: 65.136810  [244008/570908]\n",
      "loss: 74.799957  [244808/570908]\n",
      "loss: 92.793777  [245608/570908]\n",
      "loss: 66.800797  [246408/570908]\n",
      "loss: 61.396133  [247208/570908]\n",
      "loss: 58.628086  [248008/570908]\n",
      "loss: 69.345642  [248808/570908]\n",
      "loss: 88.054573  [249608/570908]\n",
      "loss: 80.302017  [250408/570908]\n",
      "loss: 98.063820  [251208/570908]\n",
      "loss: 62.216507  [252008/570908]\n",
      "loss: 58.644310  [252808/570908]\n",
      "loss: 101.218544  [253608/570908]\n",
      "loss: 52.472843  [254408/570908]\n",
      "loss: 72.878731  [255208/570908]\n",
      "loss: 43.759850  [256008/570908]\n",
      "loss: 56.488831  [256808/570908]\n",
      "loss: 67.423981  [257608/570908]\n",
      "loss: 68.443855  [258408/570908]\n",
      "loss: 94.508514  [259208/570908]\n",
      "loss: 102.746857  [260008/570908]\n",
      "loss: 71.132935  [260808/570908]\n",
      "loss: 76.006355  [261608/570908]\n",
      "loss: 76.591988  [262408/570908]\n",
      "loss: 84.534698  [263208/570908]\n",
      "loss: 62.745804  [264008/570908]\n",
      "loss: 72.727020  [264808/570908]\n",
      "loss: 64.392303  [265608/570908]\n",
      "loss: 108.073425  [266408/570908]\n",
      "loss: 65.809914  [267208/570908]\n",
      "loss: 43.921928  [268008/570908]\n",
      "loss: 53.348259  [268808/570908]\n",
      "loss: 84.098076  [269608/570908]\n",
      "loss: 69.063187  [270408/570908]\n",
      "loss: 45.633072  [271208/570908]\n",
      "loss: 51.468315  [272008/570908]\n",
      "loss: 111.853561  [272808/570908]\n",
      "loss: 66.550980  [273608/570908]\n",
      "loss: 67.745193  [274408/570908]\n",
      "loss: 58.271851  [275208/570908]\n",
      "loss: 57.503532  [276008/570908]\n",
      "loss: 59.830151  [276808/570908]\n",
      "loss: 64.299644  [277608/570908]\n",
      "loss: 67.824356  [278408/570908]\n",
      "loss: 44.670277  [279208/570908]\n",
      "loss: 73.819420  [280008/570908]\n",
      "loss: 51.789757  [280808/570908]\n",
      "loss: 41.676807  [281608/570908]\n",
      "loss: 81.163544  [282408/570908]\n",
      "loss: 67.181946  [283208/570908]\n",
      "loss: 91.420349  [284008/570908]\n",
      "loss: 76.341042  [284808/570908]\n",
      "loss: 72.166908  [285608/570908]\n",
      "loss: 70.183945  [286408/570908]\n",
      "loss: 75.690735  [287208/570908]\n",
      "loss: 79.267593  [288008/570908]\n",
      "loss: 85.556709  [288808/570908]\n",
      "loss: 89.417191  [289608/570908]\n",
      "loss: 73.096588  [290408/570908]\n",
      "loss: 72.432991  [291208/570908]\n",
      "loss: 56.858925  [292008/570908]\n",
      "loss: 80.859291  [292808/570908]\n",
      "loss: 62.604900  [293608/570908]\n",
      "loss: 71.821014  [294408/570908]\n",
      "loss: 59.232796  [295208/570908]\n",
      "loss: 50.576946  [296008/570908]\n",
      "loss: 62.297195  [296808/570908]\n",
      "loss: 73.327423  [297608/570908]\n",
      "loss: 98.564514  [298408/570908]\n",
      "loss: 61.144890  [299208/570908]\n",
      "loss: 75.065910  [300008/570908]\n",
      "loss: 75.476906  [300808/570908]\n",
      "loss: 51.845058  [301608/570908]\n",
      "loss: 61.653198  [302408/570908]\n",
      "loss: 71.079330  [303208/570908]\n",
      "loss: 60.367237  [304008/570908]\n",
      "loss: 63.271908  [304808/570908]\n",
      "loss: 45.576965  [305608/570908]\n",
      "loss: 74.820053  [306408/570908]\n",
      "loss: 45.761478  [307208/570908]\n",
      "loss: 63.328365  [308008/570908]\n",
      "loss: 71.937355  [308808/570908]\n",
      "loss: 72.277855  [309608/570908]\n",
      "loss: 66.186203  [310408/570908]\n",
      "loss: 68.174408  [311208/570908]\n",
      "loss: 76.188370  [312008/570908]\n",
      "loss: 79.306030  [312808/570908]\n",
      "loss: 64.583572  [313608/570908]\n",
      "loss: 52.335663  [314408/570908]\n",
      "loss: 48.160778  [315208/570908]\n",
      "loss: 38.595200  [316008/570908]\n",
      "loss: 90.349106  [316808/570908]\n",
      "loss: 76.644333  [317608/570908]\n",
      "loss: 88.048935  [318408/570908]\n",
      "loss: 89.277122  [319208/570908]\n",
      "loss: 68.388992  [320008/570908]\n",
      "loss: 47.559906  [320808/570908]\n",
      "loss: 55.432022  [321608/570908]\n",
      "loss: 65.631508  [322408/570908]\n",
      "loss: 63.727703  [323208/570908]\n",
      "loss: 76.012459  [324008/570908]\n",
      "loss: 54.937103  [324808/570908]\n",
      "loss: 60.881584  [325608/570908]\n",
      "loss: 58.240417  [326408/570908]\n",
      "loss: 77.580345  [327208/570908]\n",
      "loss: 75.067833  [328008/570908]\n",
      "loss: 55.748882  [328808/570908]\n",
      "loss: 56.333374  [329608/570908]\n",
      "loss: 58.590191  [330408/570908]\n",
      "loss: 66.132828  [331208/570908]\n",
      "loss: 63.177460  [332008/570908]\n",
      "loss: 74.685707  [332808/570908]\n",
      "loss: 78.876030  [333608/570908]\n",
      "loss: 39.649490  [334408/570908]\n",
      "loss: 57.102020  [335208/570908]\n",
      "loss: 74.018204  [336008/570908]\n",
      "loss: 75.842346  [336808/570908]\n",
      "loss: 69.972748  [337608/570908]\n",
      "loss: 66.113922  [338408/570908]\n",
      "loss: 68.500053  [339208/570908]\n",
      "loss: 71.799698  [340008/570908]\n",
      "loss: 65.201813  [340808/570908]\n",
      "loss: 59.692879  [341608/570908]\n",
      "loss: 68.874649  [342408/570908]\n",
      "loss: 86.977951  [343208/570908]\n",
      "loss: 97.018982  [344008/570908]\n",
      "loss: 72.621460  [344808/570908]\n",
      "loss: 57.911381  [345608/570908]\n",
      "loss: 54.707043  [346408/570908]\n",
      "loss: 65.518982  [347208/570908]\n",
      "loss: 63.808521  [348008/570908]\n",
      "loss: 74.211838  [348808/570908]\n",
      "loss: 62.766792  [349608/570908]\n",
      "loss: 64.708664  [350408/570908]\n",
      "loss: 53.950981  [351208/570908]\n",
      "loss: 72.810173  [352008/570908]\n",
      "loss: 71.612534  [352808/570908]\n",
      "loss: 60.023964  [353608/570908]\n",
      "loss: 73.117538  [354408/570908]\n",
      "loss: 66.367950  [355208/570908]\n",
      "loss: 79.821625  [356008/570908]\n",
      "loss: 59.527740  [356808/570908]\n",
      "loss: 63.762230  [357608/570908]\n",
      "loss: 55.166386  [358408/570908]\n",
      "loss: 80.621979  [359208/570908]\n",
      "loss: 67.729454  [360008/570908]\n",
      "loss: 65.575363  [360808/570908]\n",
      "loss: 74.977013  [361608/570908]\n",
      "loss: 66.031677  [362408/570908]\n",
      "loss: 62.029102  [363208/570908]\n",
      "loss: 62.939270  [364008/570908]\n",
      "loss: 52.699707  [364808/570908]\n",
      "loss: 69.327774  [365608/570908]\n",
      "loss: 52.081573  [366408/570908]\n",
      "loss: 56.557381  [367208/570908]\n",
      "loss: 65.236588  [368008/570908]\n",
      "loss: 82.040161  [368808/570908]\n",
      "loss: 83.146828  [369608/570908]\n",
      "loss: 74.744102  [370408/570908]\n",
      "loss: 67.810684  [371208/570908]\n",
      "loss: 72.470055  [372008/570908]\n",
      "loss: 68.197403  [372808/570908]\n",
      "loss: 57.786026  [373608/570908]\n",
      "loss: 71.517700  [374408/570908]\n",
      "loss: 39.940643  [375208/570908]\n",
      "loss: 58.291862  [376008/570908]\n",
      "loss: 81.853409  [376808/570908]\n",
      "loss: 46.580730  [377608/570908]\n",
      "loss: 75.527328  [378408/570908]\n",
      "loss: 55.475693  [379208/570908]\n",
      "loss: 73.767517  [380008/570908]\n",
      "loss: 86.720352  [380808/570908]\n",
      "loss: 55.596436  [381608/570908]\n",
      "loss: 61.640961  [382408/570908]\n",
      "loss: 74.219406  [383208/570908]\n",
      "loss: 80.246689  [384008/570908]\n",
      "loss: 85.359558  [384808/570908]\n",
      "loss: 71.470924  [385608/570908]\n",
      "loss: 48.854073  [386408/570908]\n",
      "loss: 71.546043  [387208/570908]\n",
      "loss: 80.295784  [388008/570908]\n",
      "loss: 64.418304  [388808/570908]\n",
      "loss: 71.794731  [389608/570908]\n",
      "loss: 72.752129  [390408/570908]\n",
      "loss: 62.720932  [391208/570908]\n",
      "loss: 84.353035  [392008/570908]\n",
      "loss: 60.260860  [392808/570908]\n",
      "loss: 43.719475  [393608/570908]\n",
      "loss: 64.656113  [394408/570908]\n",
      "loss: 58.479393  [395208/570908]\n",
      "loss: 81.822952  [396008/570908]\n",
      "loss: 74.704483  [396808/570908]\n",
      "loss: 75.537704  [397608/570908]\n",
      "loss: 67.402313  [398408/570908]\n",
      "loss: 61.132492  [399208/570908]\n",
      "loss: 70.381447  [400008/570908]\n",
      "loss: 59.232891  [400808/570908]\n",
      "loss: 89.112534  [401608/570908]\n",
      "loss: 62.001228  [402408/570908]\n",
      "loss: 65.389610  [403208/570908]\n",
      "loss: 67.514954  [404008/570908]\n",
      "loss: 69.786972  [404808/570908]\n",
      "loss: 69.188011  [405608/570908]\n",
      "loss: 82.875183  [406408/570908]\n",
      "loss: 60.206558  [407208/570908]\n",
      "loss: 62.765854  [408008/570908]\n",
      "loss: 61.431480  [408808/570908]\n",
      "loss: 47.105461  [409608/570908]\n",
      "loss: 71.705391  [410408/570908]\n",
      "loss: 50.740265  [411208/570908]\n",
      "loss: 54.782303  [412008/570908]\n",
      "loss: 61.789108  [412808/570908]\n",
      "loss: 55.851353  [413608/570908]\n",
      "loss: 62.823612  [414408/570908]\n",
      "loss: 70.711533  [415208/570908]\n",
      "loss: 58.921585  [416008/570908]\n",
      "loss: 98.433044  [416808/570908]\n",
      "loss: 59.629665  [417608/570908]\n",
      "loss: 70.378922  [418408/570908]\n",
      "loss: 39.815735  [419208/570908]\n",
      "loss: 40.041893  [420008/570908]\n",
      "loss: 61.113136  [420808/570908]\n",
      "loss: 50.929596  [421608/570908]\n",
      "loss: 68.495071  [422408/570908]\n",
      "loss: 76.844238  [423208/570908]\n",
      "loss: 50.955555  [424008/570908]\n",
      "loss: 49.966560  [424808/570908]\n",
      "loss: 66.094467  [425608/570908]\n",
      "loss: 72.040604  [426408/570908]\n",
      "loss: 72.816399  [427208/570908]\n",
      "loss: 52.556129  [428008/570908]\n",
      "loss: 65.969582  [428808/570908]\n",
      "loss: 71.658661  [429608/570908]\n",
      "loss: 65.933548  [430408/570908]\n",
      "loss: 88.078003  [431208/570908]\n",
      "loss: 89.782463  [432008/570908]\n",
      "loss: 54.870033  [432808/570908]\n",
      "loss: 71.234787  [433608/570908]\n",
      "loss: 64.436584  [434408/570908]\n",
      "loss: 57.444031  [435208/570908]\n",
      "loss: 81.633560  [436008/570908]\n",
      "loss: 117.967598  [436808/570908]\n",
      "loss: 66.324921  [437608/570908]\n",
      "loss: 81.924805  [438408/570908]\n",
      "loss: 63.802982  [439208/570908]\n",
      "loss: 49.767395  [440008/570908]\n",
      "loss: 73.692139  [440808/570908]\n",
      "loss: 64.837944  [441608/570908]\n",
      "loss: 94.483955  [442408/570908]\n",
      "loss: 81.789108  [443208/570908]\n",
      "loss: 74.137611  [444008/570908]\n",
      "loss: 76.474228  [444808/570908]\n",
      "loss: 77.374039  [445608/570908]\n",
      "loss: 56.449196  [446408/570908]\n",
      "loss: 53.973171  [447208/570908]\n",
      "loss: 59.755829  [448008/570908]\n",
      "loss: 84.968468  [448808/570908]\n",
      "loss: 80.011398  [449608/570908]\n",
      "loss: 74.783493  [450408/570908]\n",
      "loss: 78.088509  [451208/570908]\n",
      "loss: 69.797562  [452008/570908]\n",
      "loss: 40.231789  [452808/570908]\n",
      "loss: 78.846016  [453608/570908]\n",
      "loss: 96.473091  [454408/570908]\n",
      "loss: 69.887634  [455208/570908]\n",
      "loss: 60.744827  [456008/570908]\n",
      "loss: 79.449402  [456808/570908]\n",
      "loss: 63.793476  [457608/570908]\n",
      "loss: 64.147453  [458408/570908]\n",
      "loss: 50.318428  [459208/570908]\n",
      "loss: 97.032669  [460008/570908]\n",
      "loss: 65.595940  [460808/570908]\n",
      "loss: 43.823551  [461608/570908]\n",
      "loss: 44.039158  [462408/570908]\n",
      "loss: 59.505669  [463208/570908]\n",
      "loss: 60.912117  [464008/570908]\n",
      "loss: 76.326454  [464808/570908]\n",
      "loss: 53.482246  [465608/570908]\n",
      "loss: 58.596004  [466408/570908]\n",
      "loss: 56.052704  [467208/570908]\n",
      "loss: 87.011475  [468008/570908]\n",
      "loss: 64.762955  [468808/570908]\n",
      "loss: 98.258751  [469608/570908]\n",
      "loss: 76.142715  [470408/570908]\n",
      "loss: 65.081352  [471208/570908]\n",
      "loss: 65.570679  [472008/570908]\n",
      "loss: 49.519810  [472808/570908]\n",
      "loss: 56.922867  [473608/570908]\n",
      "loss: 38.776733  [474408/570908]\n",
      "loss: 94.958908  [475208/570908]\n",
      "loss: 63.718166  [476008/570908]\n",
      "loss: 63.105652  [476808/570908]\n",
      "loss: 64.356812  [477608/570908]\n",
      "loss: 55.251549  [478408/570908]\n",
      "loss: 53.104889  [479208/570908]\n",
      "loss: 85.287621  [480008/570908]\n",
      "loss: 88.409637  [480808/570908]\n",
      "loss: 54.362297  [481608/570908]\n",
      "loss: 70.541519  [482408/570908]\n",
      "loss: 82.448036  [483208/570908]\n",
      "loss: 52.261520  [484008/570908]\n",
      "loss: 59.918365  [484808/570908]\n",
      "loss: 88.700645  [485608/570908]\n",
      "loss: 71.083565  [486408/570908]\n",
      "loss: 69.393852  [487208/570908]\n",
      "loss: 60.729774  [488008/570908]\n",
      "loss: 79.129349  [488808/570908]\n",
      "loss: 37.381290  [489608/570908]\n",
      "loss: 60.273151  [490408/570908]\n",
      "loss: 56.239807  [491208/570908]\n",
      "loss: 79.898819  [492008/570908]\n",
      "loss: 67.734283  [492808/570908]\n",
      "loss: 67.529572  [493608/570908]\n",
      "loss: 57.078133  [494408/570908]\n",
      "loss: 69.657875  [495208/570908]\n",
      "loss: 61.406334  [496008/570908]\n",
      "loss: 81.209457  [496808/570908]\n",
      "loss: 58.605606  [497608/570908]\n",
      "loss: 69.134918  [498408/570908]\n",
      "loss: 47.867615  [499208/570908]\n",
      "loss: 98.378853  [500008/570908]\n",
      "loss: 72.560249  [500808/570908]\n",
      "loss: 95.822372  [501608/570908]\n",
      "loss: 73.055191  [502408/570908]\n",
      "loss: 53.722382  [503208/570908]\n",
      "loss: 72.328827  [504008/570908]\n",
      "loss: 100.816330  [504808/570908]\n",
      "loss: 87.123253  [505608/570908]\n",
      "loss: 81.068054  [506408/570908]\n",
      "loss: 75.136017  [507208/570908]\n",
      "loss: 79.693039  [508008/570908]\n",
      "loss: 65.279457  [508808/570908]\n",
      "loss: 49.044556  [509608/570908]\n",
      "loss: 77.593430  [510408/570908]\n",
      "loss: 58.738903  [511208/570908]\n",
      "loss: 48.526794  [512008/570908]\n",
      "loss: 53.680008  [512808/570908]\n",
      "loss: 61.453365  [513608/570908]\n",
      "loss: 71.858170  [514408/570908]\n",
      "loss: 91.825798  [515208/570908]\n",
      "loss: 64.427200  [516008/570908]\n",
      "loss: 60.420036  [516808/570908]\n",
      "loss: 65.468681  [517608/570908]\n",
      "loss: 69.722969  [518408/570908]\n",
      "loss: 61.552765  [519208/570908]\n",
      "loss: 99.283432  [520008/570908]\n",
      "loss: 86.209160  [520808/570908]\n",
      "loss: 72.342651  [521608/570908]\n",
      "loss: 80.899574  [522408/570908]\n",
      "loss: 54.376900  [523208/570908]\n",
      "loss: 62.597664  [524008/570908]\n",
      "loss: 68.484329  [524808/570908]\n",
      "loss: 56.076790  [525608/570908]\n",
      "loss: 71.410889  [526408/570908]\n",
      "loss: 70.552116  [527208/570908]\n",
      "loss: 63.965218  [528008/570908]\n",
      "loss: 72.534012  [528808/570908]\n",
      "loss: 34.936714  [529608/570908]\n",
      "loss: 63.591225  [530408/570908]\n",
      "loss: 66.829391  [531208/570908]\n",
      "loss: 67.187294  [532008/570908]\n",
      "loss: 60.422359  [532808/570908]\n",
      "loss: 78.602646  [533608/570908]\n",
      "loss: 59.907795  [534408/570908]\n",
      "loss: 63.832378  [535208/570908]\n",
      "loss: 77.456223  [536008/570908]\n",
      "loss: 67.106323  [536808/570908]\n",
      "loss: 94.490753  [537608/570908]\n",
      "loss: 46.731533  [538408/570908]\n",
      "loss: 50.687641  [539208/570908]\n",
      "loss: 71.689041  [540008/570908]\n",
      "loss: 59.539021  [540808/570908]\n",
      "loss: 68.710007  [541608/570908]\n",
      "loss: 75.024529  [542408/570908]\n",
      "loss: 70.492325  [543208/570908]\n",
      "loss: 86.163216  [544008/570908]\n",
      "loss: 64.019806  [544808/570908]\n",
      "loss: 60.179817  [545608/570908]\n",
      "loss: 61.429142  [546408/570908]\n",
      "loss: 96.326164  [547208/570908]\n",
      "loss: 52.449646  [548008/570908]\n",
      "loss: 52.802273  [548808/570908]\n",
      "loss: 87.710129  [549608/570908]\n",
      "loss: 63.864777  [550408/570908]\n",
      "loss: 80.192406  [551208/570908]\n",
      "loss: 66.887268  [552008/570908]\n",
      "loss: 70.691193  [552808/570908]\n",
      "loss: 64.858475  [553608/570908]\n",
      "loss: 77.924637  [554408/570908]\n",
      "loss: 69.660660  [555208/570908]\n",
      "loss: 74.481140  [556008/570908]\n",
      "loss: 52.986099  [556808/570908]\n",
      "loss: 43.929565  [557608/570908]\n",
      "loss: 53.612865  [558408/570908]\n",
      "loss: 63.258003  [559208/570908]\n",
      "loss: 37.312042  [560008/570908]\n",
      "loss: 88.586708  [560808/570908]\n",
      "loss: 51.268902  [561608/570908]\n",
      "loss: 72.076950  [562408/570908]\n",
      "loss: 58.389687  [563208/570908]\n",
      "loss: 68.836250  [564008/570908]\n",
      "loss: 67.055298  [564808/570908]\n",
      "loss: 61.469593  [565608/570908]\n",
      "loss: 58.334984  [566408/570908]\n",
      "loss: 72.334633  [567208/570908]\n",
      "loss: 93.498245  [568008/570908]\n",
      "loss: 59.849327  [568808/570908]\n",
      "loss: 80.153168  [569608/570908]\n",
      "loss: 52.353214  [570408/570908]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch (got input: [8], target: [4])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     perf_acc \u001b[38;5;241m=\u001b[39m test(test_dataloader, model, loss_function)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(perf_acc)\n",
      "Cell \u001b[0;32mIn[52], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[0;32m----> 9\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/torch/nn/modules/loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch (got input: [8], target: [4])"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "perf_timer = time.perf_counter()\n",
    "perf_acc = \"\"\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_function, optimizer)\n",
    "    perf_acc = test(test_dataloader, model, loss_function)\n",
    "    \n",
    "print(perf_acc)\n",
    "perf_timer = time.perf_counter() - perf_timer\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
