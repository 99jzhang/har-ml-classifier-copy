{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import time \n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_PATH = './data/'\n",
    "DATASET_PATH = DATA_PATH + 'uci-data/'\n",
    "MODELS_PATH = DATA_PATH + 'models/raw-models/'\n",
    "FEATURES = ['accX', 'accY', 'accZ', 'gyroX', 'gyroY', 'gyroZ', \"subject\", \"activity\"]\n",
    "\n",
    "SAMPLE_SIZE = 50000\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/lt/w0169b7x5ml3psz3nly9vj3m0000gn/T/ipykernel_40347/1212202754.py:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  '''\n",
      "/var/folders/lt/w0169b7x5ml3psz3nly9vj3m0000gn/T/ipykernel_40347/1212202754.py:9: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  complete_set = pd.read_csv(DATA_PATH + \"self-calculated/raw-data.txt\", sep='\\s+', header=None)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Load the data\n",
    "train_set = pd.read_csv(DATA_PATH + \"self-calculated/raw-train-data.txt\", sep='\\s+', header=None)\n",
    "test_set = pd.read_csv(DATA_PATH + \"self-calculated/raw-test-data.txt\", sep='\\s+', header=None)\n",
    "\n",
    "train_set.columns=FEATURES\n",
    "test_set.columns=FEATURES\n",
    "'''\n",
    "complete_set = pd.read_csv(DATA_PATH + \"self-calculated/raw-data.txt\", sep='\\s+', header=None)\n",
    "complete_set.columns = FEATURES\n",
    "train_set, test_set = train_test_split(complete_set, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model\n",
    "device = None\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "class RawDataModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(len(FEATURES) - 1, 128)  # Adjust input size based on output of Conv1d\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.batchnorm1 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(0.05)\n",
    "\n",
    "        self.fc2 = nn.Linear(128, 64)  # Adjust input size based on output of Conv1d\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(0.05)\n",
    "\n",
    "        self.fc3 = nn.Linear(64, 32)  # Adjust input size based on output of Conv1d\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.batchnorm3 = nn.BatchNorm1d(32)\n",
    "        self.dropout3 = nn.Dropout(0.05)\n",
    "        \n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.fc5 = nn.Linear(16, 12)\n",
    "        self.softmax = nn.Softmax(dim=1)  # Use dim=1 for multi-class classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)           # Shape: [batch_size, 512]\n",
    "        x = self.relu1(x)         # Apply ReLU activation\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.dropout1(x)       # Apply dropout\n",
    "\n",
    "        x = self.fc2(x)           # Shape: [batch_size, 512]\n",
    "        x = self.relu2(x)         # Apply ReLU activation\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout2(x)       # Apply dropout\n",
    "\n",
    "        x = self.fc3(x)           # Shape: [batch_size, 512]\n",
    "        x = self.relu3(x)         # Apply ReLU activation\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.dropout3(x)       # Apply dropout\n",
    "\n",
    "        x = self.fc4(x)           # Shape: [batch_size, 32]\n",
    "        x = self.silu(x)          # Apply SiLU activation\n",
    "        x = self.fc5(x)           # Shape: [batch_size, 8]\n",
    "        x = self.softmax(x)       # Apply Softmax for classification\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = RawDataModel().to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing function\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    # Get batch num\n",
    "    num_batches = len(dataloader.dataset) / BATCH_SIZE\n",
    "    i = 0\n",
    "\n",
    "    # Set the model to train mode\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        i += 1\n",
    "        if batch % 50 == 0:\n",
    "            print(f\"loss: {loss.item()}, batch: {i} out of {math.ceil(num_batches)}\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    return(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset preparation\n",
    "class HAPTDataset(Dataset):\n",
    "    def __init__(self, dataset, features, label):\n",
    "        self.data = torch.tensor(dataset[features].values, dtype=torch.float32)[:SAMPLE_SIZE]\n",
    "        self.labels = torch.tensor(dataset[label].values, dtype=torch.float32)[:SAMPLE_SIZE]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "    \n",
    "train_dataset = HAPTDataset(train_set, train_set.columns[:-1], 'activity')\n",
    "test_dataset = HAPTDataset(test_set, test_set.columns[:-1], 'activity')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.4043140411376953, batch: 1 out of 1563\n",
      "loss: 2.4763245582580566, batch: 51 out of 1563\n",
      "loss: 2.471712112426758, batch: 101 out of 1563\n",
      "loss: 2.313426971435547, batch: 151 out of 1563\n",
      "loss: 2.4518556594848633, batch: 201 out of 1563\n",
      "loss: 2.444446325302124, batch: 251 out of 1563\n",
      "loss: 2.3938539028167725, batch: 301 out of 1563\n",
      "loss: 2.2776756286621094, batch: 351 out of 1563\n",
      "loss: 2.3186991214752197, batch: 401 out of 1563\n",
      "loss: 2.2698001861572266, batch: 451 out of 1563\n",
      "loss: 2.3349740505218506, batch: 501 out of 1563\n",
      "loss: 2.30187726020813, batch: 551 out of 1563\n",
      "loss: 2.3258752822875977, batch: 601 out of 1563\n",
      "loss: 2.255519390106201, batch: 651 out of 1563\n",
      "loss: 2.2405619621276855, batch: 701 out of 1563\n",
      "loss: 2.375469923019409, batch: 751 out of 1563\n",
      "loss: 2.31070876121521, batch: 801 out of 1563\n",
      "loss: 2.3142518997192383, batch: 851 out of 1563\n",
      "loss: 2.228661060333252, batch: 901 out of 1563\n",
      "loss: 2.2780795097351074, batch: 951 out of 1563\n",
      "loss: 2.244011878967285, batch: 1001 out of 1563\n",
      "loss: 2.207115650177002, batch: 1051 out of 1563\n",
      "loss: 2.151651382446289, batch: 1101 out of 1563\n",
      "loss: 1.9588623046875, batch: 1151 out of 1563\n",
      "loss: 2.1000208854675293, batch: 1201 out of 1563\n",
      "loss: 2.114643096923828, batch: 1251 out of 1563\n",
      "loss: 2.140258312225342, batch: 1301 out of 1563\n",
      "loss: 2.112309455871582, batch: 1351 out of 1563\n",
      "loss: 2.120211601257324, batch: 1401 out of 1563\n",
      "loss: 2.1484909057617188, batch: 1451 out of 1563\n",
      "loss: 2.241563320159912, batch: 1501 out of 1563\n",
      "loss: 2.1737060546875, batch: 1551 out of 1563\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.9790685176849365, batch: 1 out of 1563\n",
      "loss: 2.1956067085266113, batch: 51 out of 1563\n",
      "loss: 2.135885715484619, batch: 101 out of 1563\n",
      "loss: 2.1831977367401123, batch: 151 out of 1563\n",
      "loss: 2.1273179054260254, batch: 201 out of 1563\n",
      "loss: 2.062842845916748, batch: 251 out of 1563\n",
      "loss: 1.9446442127227783, batch: 301 out of 1563\n",
      "loss: 2.026076316833496, batch: 351 out of 1563\n",
      "loss: 2.133836507797241, batch: 401 out of 1563\n",
      "loss: 2.0602307319641113, batch: 451 out of 1563\n",
      "loss: 2.091954231262207, batch: 501 out of 1563\n",
      "loss: 1.9455188512802124, batch: 551 out of 1563\n",
      "loss: 2.091824531555176, batch: 601 out of 1563\n",
      "loss: 1.8467974662780762, batch: 651 out of 1563\n",
      "loss: 2.106560230255127, batch: 701 out of 1563\n",
      "loss: 1.9584693908691406, batch: 751 out of 1563\n",
      "loss: 2.013934373855591, batch: 801 out of 1563\n",
      "loss: 2.0808064937591553, batch: 851 out of 1563\n",
      "loss: 2.120328903198242, batch: 901 out of 1563\n",
      "loss: 1.8848239183425903, batch: 951 out of 1563\n",
      "loss: 2.008091449737549, batch: 1001 out of 1563\n",
      "loss: 1.9185750484466553, batch: 1051 out of 1563\n",
      "loss: 2.120944023132324, batch: 1101 out of 1563\n",
      "loss: 1.9724081754684448, batch: 1151 out of 1563\n",
      "loss: 2.1043715476989746, batch: 1201 out of 1563\n",
      "loss: 2.017594814300537, batch: 1251 out of 1563\n",
      "loss: 2.0131285190582275, batch: 1301 out of 1563\n",
      "loss: 2.087416887283325, batch: 1351 out of 1563\n",
      "loss: 1.8890819549560547, batch: 1401 out of 1563\n",
      "loss: 2.015799045562744, batch: 1451 out of 1563\n",
      "loss: 1.9601755142211914, batch: 1501 out of 1563\n",
      "loss: 1.9159879684448242, batch: 1551 out of 1563\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.9186172485351562, batch: 1 out of 1563\n",
      "loss: 2.034839630126953, batch: 51 out of 1563\n",
      "loss: 1.8384568691253662, batch: 101 out of 1563\n",
      "loss: 1.9751496315002441, batch: 151 out of 1563\n",
      "loss: 1.930274486541748, batch: 201 out of 1563\n",
      "loss: 2.0042316913604736, batch: 251 out of 1563\n",
      "loss: 1.9759740829467773, batch: 301 out of 1563\n",
      "loss: 2.0356690883636475, batch: 351 out of 1563\n",
      "loss: 1.8747727870941162, batch: 401 out of 1563\n",
      "loss: 2.0852108001708984, batch: 451 out of 1563\n",
      "loss: 2.004558563232422, batch: 501 out of 1563\n",
      "loss: 1.9949243068695068, batch: 551 out of 1563\n",
      "loss: 2.0163955688476562, batch: 601 out of 1563\n",
      "loss: 1.8502497673034668, batch: 651 out of 1563\n",
      "loss: 1.9703902006149292, batch: 701 out of 1563\n",
      "loss: 1.969695806503296, batch: 751 out of 1563\n",
      "loss: 1.9518729448318481, batch: 801 out of 1563\n",
      "loss: 1.9245519638061523, batch: 851 out of 1563\n",
      "loss: 1.9972009658813477, batch: 901 out of 1563\n",
      "loss: 1.9889161586761475, batch: 951 out of 1563\n",
      "loss: 1.8776568174362183, batch: 1001 out of 1563\n",
      "loss: 2.050319194793701, batch: 1051 out of 1563\n",
      "loss: 2.0332651138305664, batch: 1101 out of 1563\n",
      "loss: 2.0302343368530273, batch: 1151 out of 1563\n",
      "loss: 2.1351442337036133, batch: 1201 out of 1563\n",
      "loss: 1.933652639389038, batch: 1251 out of 1563\n",
      "loss: 1.9710581302642822, batch: 1301 out of 1563\n",
      "loss: 2.0112242698669434, batch: 1351 out of 1563\n",
      "loss: 2.1856014728546143, batch: 1401 out of 1563\n",
      "loss: 1.9030073881149292, batch: 1451 out of 1563\n",
      "loss: 2.044614791870117, batch: 1501 out of 1563\n",
      "loss: 1.8871002197265625, batch: 1551 out of 1563\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.0157742500305176, batch: 1 out of 1563\n",
      "loss: 2.034050941467285, batch: 51 out of 1563\n",
      "loss: 2.005826950073242, batch: 101 out of 1563\n",
      "loss: 2.101020097732544, batch: 151 out of 1563\n",
      "loss: 1.8725738525390625, batch: 201 out of 1563\n",
      "loss: 2.0956766605377197, batch: 251 out of 1563\n",
      "loss: 1.8876209259033203, batch: 301 out of 1563\n",
      "loss: 2.0638837814331055, batch: 351 out of 1563\n",
      "loss: 1.8923290967941284, batch: 401 out of 1563\n",
      "loss: 1.6913020610809326, batch: 451 out of 1563\n",
      "loss: 2.0151665210723877, batch: 501 out of 1563\n",
      "loss: 2.086850643157959, batch: 551 out of 1563\n",
      "loss: 1.9244065284729004, batch: 601 out of 1563\n",
      "loss: 1.9098269939422607, batch: 651 out of 1563\n",
      "loss: 1.964759349822998, batch: 701 out of 1563\n",
      "loss: 1.8994512557983398, batch: 751 out of 1563\n",
      "loss: 1.97727632522583, batch: 801 out of 1563\n",
      "loss: 1.861194133758545, batch: 851 out of 1563\n",
      "loss: 1.9931164979934692, batch: 901 out of 1563\n",
      "loss: 1.9120368957519531, batch: 951 out of 1563\n",
      "loss: 1.9586687088012695, batch: 1001 out of 1563\n",
      "loss: 2.0665853023529053, batch: 1051 out of 1563\n",
      "loss: 1.9491462707519531, batch: 1101 out of 1563\n",
      "loss: 2.125080108642578, batch: 1151 out of 1563\n",
      "loss: 1.8387291431427002, batch: 1201 out of 1563\n",
      "loss: 1.8415603637695312, batch: 1251 out of 1563\n",
      "loss: 2.003373622894287, batch: 1301 out of 1563\n",
      "loss: 1.8871582746505737, batch: 1351 out of 1563\n",
      "loss: 2.0629782676696777, batch: 1401 out of 1563\n",
      "loss: 1.8586747646331787, batch: 1451 out of 1563\n",
      "loss: 1.7957086563110352, batch: 1501 out of 1563\n",
      "loss: 1.9137545824050903, batch: 1551 out of 1563\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.081247329711914, batch: 1 out of 1563\n",
      "loss: 1.9111762046813965, batch: 51 out of 1563\n",
      "loss: 1.9758052825927734, batch: 101 out of 1563\n",
      "loss: 1.822357416152954, batch: 151 out of 1563\n",
      "loss: 2.0461926460266113, batch: 201 out of 1563\n",
      "loss: 2.107626438140869, batch: 251 out of 1563\n",
      "loss: 1.9035508632659912, batch: 301 out of 1563\n",
      "loss: 1.9950637817382812, batch: 351 out of 1563\n",
      "loss: 1.9782544374465942, batch: 401 out of 1563\n",
      "loss: 1.9086415767669678, batch: 451 out of 1563\n",
      "loss: 1.9821953773498535, batch: 501 out of 1563\n",
      "loss: 1.8870971202850342, batch: 551 out of 1563\n",
      "loss: 1.8971296548843384, batch: 601 out of 1563\n",
      "loss: 2.0270323753356934, batch: 651 out of 1563\n",
      "loss: 1.9645345211029053, batch: 701 out of 1563\n",
      "loss: 1.9213789701461792, batch: 751 out of 1563\n",
      "loss: 1.969855546951294, batch: 801 out of 1563\n",
      "loss: 1.895221471786499, batch: 851 out of 1563\n",
      "loss: 1.942830204963684, batch: 901 out of 1563\n",
      "loss: 2.108686923980713, batch: 951 out of 1563\n",
      "loss: 1.9008684158325195, batch: 1001 out of 1563\n",
      "loss: 1.9356377124786377, batch: 1051 out of 1563\n",
      "loss: 2.0318448543548584, batch: 1101 out of 1563\n",
      "loss: 2.190593719482422, batch: 1151 out of 1563\n",
      "loss: 1.9224903583526611, batch: 1201 out of 1563\n",
      "loss: 1.896338701248169, batch: 1251 out of 1563\n",
      "loss: 1.8496613502502441, batch: 1301 out of 1563\n",
      "loss: 1.8067266941070557, batch: 1351 out of 1563\n",
      "loss: 1.8708107471466064, batch: 1401 out of 1563\n",
      "loss: 1.9726871252059937, batch: 1451 out of 1563\n",
      "loss: 1.9577066898345947, batch: 1501 out of 1563\n",
      "loss: 1.980576992034912, batch: 1551 out of 1563\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.977968692779541, batch: 1 out of 1563\n",
      "loss: 2.0739998817443848, batch: 51 out of 1563\n",
      "loss: 1.977821946144104, batch: 101 out of 1563\n",
      "loss: 1.9746830463409424, batch: 151 out of 1563\n",
      "loss: 1.9282548427581787, batch: 201 out of 1563\n",
      "loss: 2.0524935722351074, batch: 251 out of 1563\n",
      "loss: 1.8752672672271729, batch: 301 out of 1563\n",
      "loss: 1.940359115600586, batch: 351 out of 1563\n",
      "loss: 1.9229665994644165, batch: 401 out of 1563\n",
      "loss: 1.9177846908569336, batch: 451 out of 1563\n",
      "loss: 2.0344622135162354, batch: 501 out of 1563\n",
      "loss: 1.9423882961273193, batch: 551 out of 1563\n",
      "loss: 2.0452795028686523, batch: 601 out of 1563\n",
      "loss: 1.8697099685668945, batch: 651 out of 1563\n",
      "loss: 1.9536535739898682, batch: 701 out of 1563\n",
      "loss: 2.011521577835083, batch: 751 out of 1563\n",
      "loss: 1.9243546724319458, batch: 801 out of 1563\n",
      "loss: 1.9663347005844116, batch: 851 out of 1563\n",
      "loss: 1.8264055252075195, batch: 901 out of 1563\n",
      "loss: 2.0370607376098633, batch: 951 out of 1563\n",
      "loss: 1.8596010208129883, batch: 1001 out of 1563\n",
      "loss: 2.025904893875122, batch: 1051 out of 1563\n",
      "loss: 1.9839448928833008, batch: 1101 out of 1563\n",
      "loss: 1.7875860929489136, batch: 1151 out of 1563\n",
      "loss: 1.9038424491882324, batch: 1201 out of 1563\n",
      "loss: 2.0545310974121094, batch: 1251 out of 1563\n",
      "loss: 1.98972749710083, batch: 1301 out of 1563\n",
      "loss: 2.0743179321289062, batch: 1351 out of 1563\n",
      "loss: 1.8997299671173096, batch: 1401 out of 1563\n",
      "loss: 2.0182535648345947, batch: 1451 out of 1563\n",
      "loss: 2.0355536937713623, batch: 1501 out of 1563\n",
      "loss: 1.8429127931594849, batch: 1551 out of 1563\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.916395664215088, batch: 1 out of 1563\n",
      "loss: 1.8945302963256836, batch: 51 out of 1563\n",
      "loss: 1.8561654090881348, batch: 101 out of 1563\n",
      "loss: 1.9597550630569458, batch: 151 out of 1563\n",
      "loss: 2.032838821411133, batch: 201 out of 1563\n",
      "loss: 1.9433480501174927, batch: 251 out of 1563\n",
      "loss: 2.059372901916504, batch: 301 out of 1563\n",
      "loss: 1.6417957544326782, batch: 351 out of 1563\n",
      "loss: 1.7745258808135986, batch: 401 out of 1563\n",
      "loss: 1.7823052406311035, batch: 451 out of 1563\n",
      "loss: 2.080138921737671, batch: 501 out of 1563\n",
      "loss: 2.0131869316101074, batch: 551 out of 1563\n",
      "loss: 1.9688951969146729, batch: 601 out of 1563\n",
      "loss: 1.996490240097046, batch: 651 out of 1563\n",
      "loss: 2.0113470554351807, batch: 701 out of 1563\n",
      "loss: 1.9395278692245483, batch: 751 out of 1563\n",
      "loss: 1.9897091388702393, batch: 801 out of 1563\n",
      "loss: 2.0006604194641113, batch: 851 out of 1563\n",
      "loss: 2.0178213119506836, batch: 901 out of 1563\n",
      "loss: 2.0115203857421875, batch: 951 out of 1563\n",
      "loss: 1.925028681755066, batch: 1001 out of 1563\n",
      "loss: 2.0580010414123535, batch: 1051 out of 1563\n",
      "loss: 1.972724437713623, batch: 1101 out of 1563\n",
      "loss: 1.9691084623336792, batch: 1151 out of 1563\n",
      "loss: 1.9222625494003296, batch: 1201 out of 1563\n",
      "loss: 1.858968734741211, batch: 1251 out of 1563\n",
      "loss: 1.868373155593872, batch: 1301 out of 1563\n",
      "loss: 1.9421206712722778, batch: 1351 out of 1563\n",
      "loss: 1.8830537796020508, batch: 1401 out of 1563\n",
      "loss: 1.8518614768981934, batch: 1451 out of 1563\n",
      "loss: 1.936129093170166, batch: 1501 out of 1563\n",
      "loss: 1.9941577911376953, batch: 1551 out of 1563\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.9004111289978027, batch: 1 out of 1563\n",
      "loss: 1.8778200149536133, batch: 51 out of 1563\n",
      "loss: 1.7626502513885498, batch: 101 out of 1563\n",
      "loss: 1.8303935527801514, batch: 151 out of 1563\n",
      "loss: 2.053344488143921, batch: 201 out of 1563\n",
      "loss: 1.959805965423584, batch: 251 out of 1563\n",
      "loss: 1.8565089702606201, batch: 301 out of 1563\n",
      "loss: 1.8948084115982056, batch: 351 out of 1563\n",
      "loss: 1.862492322921753, batch: 401 out of 1563\n",
      "loss: 1.771634817123413, batch: 451 out of 1563\n",
      "loss: 2.0396900177001953, batch: 501 out of 1563\n",
      "loss: 1.8947497606277466, batch: 551 out of 1563\n",
      "loss: 2.0673398971557617, batch: 601 out of 1563\n",
      "loss: 2.0306031703948975, batch: 651 out of 1563\n",
      "loss: 1.762056827545166, batch: 701 out of 1563\n",
      "loss: 1.9609951972961426, batch: 751 out of 1563\n",
      "loss: 1.9684908390045166, batch: 801 out of 1563\n",
      "loss: 1.8762991428375244, batch: 851 out of 1563\n",
      "loss: 1.7928842306137085, batch: 901 out of 1563\n",
      "loss: 1.913122296333313, batch: 951 out of 1563\n",
      "loss: 2.0166468620300293, batch: 1001 out of 1563\n",
      "loss: 1.800593614578247, batch: 1051 out of 1563\n",
      "loss: 2.1311545372009277, batch: 1101 out of 1563\n",
      "loss: 2.1141982078552246, batch: 1151 out of 1563\n",
      "loss: 1.8189148902893066, batch: 1201 out of 1563\n",
      "loss: 1.9097824096679688, batch: 1251 out of 1563\n",
      "loss: 1.9300754070281982, batch: 1301 out of 1563\n",
      "loss: 1.972888708114624, batch: 1351 out of 1563\n",
      "loss: 1.833881139755249, batch: 1401 out of 1563\n",
      "loss: 1.935937762260437, batch: 1451 out of 1563\n",
      "loss: 1.9774928092956543, batch: 1501 out of 1563\n",
      "loss: 2.055586099624634, batch: 1551 out of 1563\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.024397611618042, batch: 1 out of 1563\n",
      "loss: 1.9117333889007568, batch: 51 out of 1563\n",
      "loss: 2.1635823249816895, batch: 101 out of 1563\n",
      "loss: 1.820603847503662, batch: 151 out of 1563\n",
      "loss: 1.9281163215637207, batch: 201 out of 1563\n",
      "loss: 2.0228030681610107, batch: 251 out of 1563\n",
      "loss: 1.8964258432388306, batch: 301 out of 1563\n",
      "loss: 2.266136646270752, batch: 351 out of 1563\n",
      "loss: 2.0124621391296387, batch: 401 out of 1563\n",
      "loss: 1.932671308517456, batch: 451 out of 1563\n",
      "loss: 1.878852128982544, batch: 501 out of 1563\n",
      "loss: 1.9487340450286865, batch: 551 out of 1563\n",
      "loss: 1.8179619312286377, batch: 601 out of 1563\n",
      "loss: 2.057199001312256, batch: 651 out of 1563\n",
      "loss: 1.8777822256088257, batch: 701 out of 1563\n",
      "loss: 1.948509693145752, batch: 751 out of 1563\n",
      "loss: 2.092830181121826, batch: 801 out of 1563\n",
      "loss: 1.8515101671218872, batch: 851 out of 1563\n",
      "loss: 1.7977217435836792, batch: 901 out of 1563\n",
      "loss: 1.9027886390686035, batch: 951 out of 1563\n",
      "loss: 1.86800217628479, batch: 1001 out of 1563\n",
      "loss: 1.9075113534927368, batch: 1051 out of 1563\n",
      "loss: 1.9189949035644531, batch: 1101 out of 1563\n",
      "loss: 2.0741848945617676, batch: 1151 out of 1563\n",
      "loss: 1.8216907978057861, batch: 1201 out of 1563\n",
      "loss: 1.9259941577911377, batch: 1251 out of 1563\n",
      "loss: 2.003427505493164, batch: 1301 out of 1563\n",
      "loss: 1.923685073852539, batch: 1351 out of 1563\n",
      "loss: 1.786217451095581, batch: 1401 out of 1563\n",
      "loss: 1.9480206966400146, batch: 1451 out of 1563\n",
      "loss: 1.9001054763793945, batch: 1501 out of 1563\n",
      "loss: 1.9509843587875366, batch: 1551 out of 1563\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.0446815490722656, batch: 1 out of 1563\n",
      "loss: 1.8919553756713867, batch: 51 out of 1563\n",
      "loss: 1.8589210510253906, batch: 101 out of 1563\n",
      "loss: 1.926068902015686, batch: 151 out of 1563\n",
      "loss: 1.9487134218215942, batch: 201 out of 1563\n",
      "loss: 1.9373458623886108, batch: 251 out of 1563\n",
      "loss: 1.9794130325317383, batch: 301 out of 1563\n",
      "loss: 1.9696569442749023, batch: 351 out of 1563\n",
      "loss: 1.9312931299209595, batch: 401 out of 1563\n",
      "loss: 1.9250746965408325, batch: 451 out of 1563\n",
      "loss: 1.9365150928497314, batch: 501 out of 1563\n",
      "loss: 1.9542278051376343, batch: 551 out of 1563\n",
      "loss: 1.8757948875427246, batch: 601 out of 1563\n",
      "loss: 1.970242977142334, batch: 651 out of 1563\n",
      "loss: 2.093963146209717, batch: 701 out of 1563\n",
      "loss: 1.849078893661499, batch: 751 out of 1563\n",
      "loss: 1.9505144357681274, batch: 801 out of 1563\n",
      "loss: 1.976659893989563, batch: 851 out of 1563\n",
      "loss: 2.0908098220825195, batch: 901 out of 1563\n",
      "loss: 1.9687252044677734, batch: 951 out of 1563\n",
      "loss: 1.941595196723938, batch: 1001 out of 1563\n",
      "loss: 1.846553921699524, batch: 1051 out of 1563\n",
      "loss: 1.9177725315093994, batch: 1101 out of 1563\n",
      "loss: 1.8782329559326172, batch: 1151 out of 1563\n",
      "loss: 1.9500499963760376, batch: 1201 out of 1563\n",
      "loss: 1.8713356256484985, batch: 1251 out of 1563\n",
      "loss: 1.9561257362365723, batch: 1301 out of 1563\n",
      "loss: 1.9963281154632568, batch: 1351 out of 1563\n",
      "loss: 1.8470265865325928, batch: 1401 out of 1563\n",
      "loss: 1.8478879928588867, batch: 1451 out of 1563\n",
      "loss: 2.0860843658447266, batch: 1501 out of 1563\n",
      "loss: 1.8790576457977295, batch: 1551 out of 1563\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.8857934474945068, batch: 1 out of 1563\n",
      "loss: 2.054502487182617, batch: 51 out of 1563\n",
      "loss: 1.7691270112991333, batch: 101 out of 1563\n",
      "loss: 1.8831768035888672, batch: 151 out of 1563\n",
      "loss: 1.8088523149490356, batch: 201 out of 1563\n",
      "loss: 1.8134217262268066, batch: 251 out of 1563\n",
      "loss: 1.7737784385681152, batch: 301 out of 1563\n",
      "loss: 1.8327393531799316, batch: 351 out of 1563\n",
      "loss: 1.9152323007583618, batch: 401 out of 1563\n",
      "loss: 1.9081567525863647, batch: 451 out of 1563\n",
      "loss: 1.9160078763961792, batch: 501 out of 1563\n",
      "loss: 1.8211735486984253, batch: 551 out of 1563\n",
      "loss: 1.977455496788025, batch: 601 out of 1563\n",
      "loss: 2.0055012702941895, batch: 651 out of 1563\n",
      "loss: 1.9996581077575684, batch: 701 out of 1563\n",
      "loss: 1.9389190673828125, batch: 751 out of 1563\n",
      "loss: 2.0912556648254395, batch: 801 out of 1563\n",
      "loss: 1.9574838876724243, batch: 851 out of 1563\n",
      "loss: 1.9505925178527832, batch: 901 out of 1563\n",
      "loss: 1.9187653064727783, batch: 951 out of 1563\n",
      "loss: 1.993899941444397, batch: 1001 out of 1563\n",
      "loss: 1.86985445022583, batch: 1051 out of 1563\n",
      "loss: 1.747520089149475, batch: 1101 out of 1563\n",
      "loss: 1.9516041278839111, batch: 1151 out of 1563\n",
      "loss: 1.8158366680145264, batch: 1201 out of 1563\n",
      "loss: 1.8301622867584229, batch: 1251 out of 1563\n",
      "loss: 1.9601356983184814, batch: 1301 out of 1563\n",
      "loss: 2.0665171146392822, batch: 1351 out of 1563\n",
      "loss: 1.8972961902618408, batch: 1401 out of 1563\n",
      "loss: 1.8236252069473267, batch: 1451 out of 1563\n",
      "loss: 1.8669629096984863, batch: 1501 out of 1563\n",
      "loss: 1.9238295555114746, batch: 1551 out of 1563\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.9780834913253784, batch: 1 out of 1563\n",
      "loss: 1.8455089330673218, batch: 51 out of 1563\n",
      "loss: 1.9177137613296509, batch: 101 out of 1563\n",
      "loss: 1.9510293006896973, batch: 151 out of 1563\n",
      "loss: 1.8516346216201782, batch: 201 out of 1563\n",
      "loss: 1.7894678115844727, batch: 251 out of 1563\n",
      "loss: 1.9014753103256226, batch: 301 out of 1563\n",
      "loss: 1.8596012592315674, batch: 351 out of 1563\n",
      "loss: 1.92732572555542, batch: 401 out of 1563\n",
      "loss: 1.8728832006454468, batch: 451 out of 1563\n",
      "loss: 2.0023975372314453, batch: 501 out of 1563\n",
      "loss: 1.8640544414520264, batch: 551 out of 1563\n",
      "loss: 1.8812711238861084, batch: 601 out of 1563\n",
      "loss: 2.0018227100372314, batch: 651 out of 1563\n",
      "loss: 1.9596831798553467, batch: 701 out of 1563\n",
      "loss: 1.740708589553833, batch: 751 out of 1563\n",
      "loss: 1.9414591789245605, batch: 801 out of 1563\n",
      "loss: 1.8458316326141357, batch: 851 out of 1563\n",
      "loss: 1.8705470561981201, batch: 901 out of 1563\n",
      "loss: 2.0386924743652344, batch: 951 out of 1563\n",
      "loss: 1.9330203533172607, batch: 1001 out of 1563\n",
      "loss: 1.874983310699463, batch: 1051 out of 1563\n",
      "loss: 1.8779270648956299, batch: 1101 out of 1563\n",
      "loss: 1.8053007125854492, batch: 1151 out of 1563\n",
      "loss: 2.0104408264160156, batch: 1201 out of 1563\n",
      "loss: 1.9635484218597412, batch: 1251 out of 1563\n",
      "loss: 1.9223865270614624, batch: 1301 out of 1563\n",
      "loss: 1.8288190364837646, batch: 1351 out of 1563\n",
      "loss: 1.8748178482055664, batch: 1401 out of 1563\n",
      "loss: 1.7755542993545532, batch: 1451 out of 1563\n",
      "loss: 1.9278978109359741, batch: 1501 out of 1563\n",
      "loss: 1.9962310791015625, batch: 1551 out of 1563\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.8959230184555054, batch: 1 out of 1563\n",
      "loss: 1.9923934936523438, batch: 51 out of 1563\n",
      "loss: 1.872809648513794, batch: 101 out of 1563\n",
      "loss: 2.0848796367645264, batch: 151 out of 1563\n",
      "loss: 1.9917681217193604, batch: 201 out of 1563\n",
      "loss: 1.7020165920257568, batch: 251 out of 1563\n",
      "loss: 1.9377739429473877, batch: 301 out of 1563\n",
      "loss: 1.8672168254852295, batch: 351 out of 1563\n",
      "loss: 1.969109058380127, batch: 401 out of 1563\n",
      "loss: 1.951085090637207, batch: 451 out of 1563\n",
      "loss: 1.8037631511688232, batch: 501 out of 1563\n",
      "loss: 1.9335963726043701, batch: 551 out of 1563\n",
      "loss: 1.8111655712127686, batch: 601 out of 1563\n",
      "loss: 1.9431226253509521, batch: 651 out of 1563\n",
      "loss: 1.9648890495300293, batch: 701 out of 1563\n",
      "loss: 1.796204924583435, batch: 751 out of 1563\n",
      "loss: 1.9262946844100952, batch: 801 out of 1563\n",
      "loss: 2.070753574371338, batch: 851 out of 1563\n",
      "loss: 1.7559285163879395, batch: 901 out of 1563\n",
      "loss: 1.9431517124176025, batch: 951 out of 1563\n",
      "loss: 1.973116397857666, batch: 1001 out of 1563\n",
      "loss: 1.96686589717865, batch: 1051 out of 1563\n",
      "loss: 1.9349687099456787, batch: 1101 out of 1563\n",
      "loss: 1.7607696056365967, batch: 1151 out of 1563\n",
      "loss: 1.829756259918213, batch: 1201 out of 1563\n",
      "loss: 1.9585566520690918, batch: 1251 out of 1563\n",
      "loss: 1.9235103130340576, batch: 1301 out of 1563\n",
      "loss: 1.8748682737350464, batch: 1351 out of 1563\n",
      "loss: 1.8900141716003418, batch: 1401 out of 1563\n",
      "loss: 1.9326879978179932, batch: 1451 out of 1563\n",
      "loss: 1.9016287326812744, batch: 1501 out of 1563\n",
      "loss: 1.8279190063476562, batch: 1551 out of 1563\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.908031940460205, batch: 1 out of 1563\n",
      "loss: 1.893847942352295, batch: 51 out of 1563\n",
      "loss: 1.8109564781188965, batch: 101 out of 1563\n",
      "loss: 1.9029408693313599, batch: 151 out of 1563\n",
      "loss: 1.9614293575286865, batch: 201 out of 1563\n",
      "loss: 1.8737549781799316, batch: 251 out of 1563\n",
      "loss: 1.8789966106414795, batch: 301 out of 1563\n",
      "loss: 1.850355625152588, batch: 351 out of 1563\n",
      "loss: 2.1318607330322266, batch: 401 out of 1563\n",
      "loss: 2.0800390243530273, batch: 451 out of 1563\n",
      "loss: 1.9744341373443604, batch: 501 out of 1563\n",
      "loss: 1.950913667678833, batch: 551 out of 1563\n",
      "loss: 1.908237099647522, batch: 601 out of 1563\n",
      "loss: 1.9610975980758667, batch: 651 out of 1563\n",
      "loss: 1.9588065147399902, batch: 701 out of 1563\n",
      "loss: 1.8585809469223022, batch: 751 out of 1563\n",
      "loss: 1.8514580726623535, batch: 801 out of 1563\n",
      "loss: 1.9635517597198486, batch: 851 out of 1563\n",
      "loss: 1.856623649597168, batch: 901 out of 1563\n",
      "loss: 2.0234100818634033, batch: 951 out of 1563\n",
      "loss: 1.9521276950836182, batch: 1001 out of 1563\n",
      "loss: 1.9972436428070068, batch: 1051 out of 1563\n",
      "loss: 1.9646379947662354, batch: 1101 out of 1563\n",
      "loss: 2.0314483642578125, batch: 1151 out of 1563\n",
      "loss: 1.9881888628005981, batch: 1201 out of 1563\n",
      "loss: 1.9961808919906616, batch: 1251 out of 1563\n",
      "loss: 1.903843641281128, batch: 1301 out of 1563\n",
      "loss: 1.990585207939148, batch: 1351 out of 1563\n",
      "loss: 1.8010458946228027, batch: 1401 out of 1563\n",
      "loss: 1.7661831378936768, batch: 1451 out of 1563\n",
      "loss: 1.9268972873687744, batch: 1501 out of 1563\n",
      "loss: 1.9679336547851562, batch: 1551 out of 1563\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.812471628189087, batch: 1 out of 1563\n",
      "loss: 1.8128550052642822, batch: 51 out of 1563\n",
      "loss: 1.852203607559204, batch: 101 out of 1563\n",
      "loss: 1.9095045328140259, batch: 151 out of 1563\n",
      "loss: 1.906148076057434, batch: 201 out of 1563\n",
      "loss: 1.861037254333496, batch: 251 out of 1563\n",
      "loss: 2.014331340789795, batch: 301 out of 1563\n",
      "loss: 1.8155484199523926, batch: 351 out of 1563\n",
      "loss: 1.7985970973968506, batch: 401 out of 1563\n",
      "loss: 2.0841588973999023, batch: 451 out of 1563\n",
      "loss: 1.9317737817764282, batch: 501 out of 1563\n",
      "loss: 1.975257396697998, batch: 551 out of 1563\n",
      "loss: 1.9459470510482788, batch: 601 out of 1563\n",
      "loss: 1.8814893960952759, batch: 651 out of 1563\n",
      "loss: 1.951341152191162, batch: 701 out of 1563\n",
      "loss: 1.743319034576416, batch: 751 out of 1563\n",
      "loss: 1.7876152992248535, batch: 801 out of 1563\n",
      "loss: 1.8591649532318115, batch: 851 out of 1563\n",
      "loss: 1.9626840353012085, batch: 901 out of 1563\n",
      "loss: 2.140897035598755, batch: 951 out of 1563\n",
      "loss: 1.740825891494751, batch: 1001 out of 1563\n",
      "loss: 1.9126825332641602, batch: 1051 out of 1563\n",
      "loss: 1.8072102069854736, batch: 1101 out of 1563\n",
      "loss: 1.8625330924987793, batch: 1151 out of 1563\n",
      "loss: 1.8938052654266357, batch: 1201 out of 1563\n",
      "loss: 1.9331343173980713, batch: 1251 out of 1563\n",
      "loss: 1.8809870481491089, batch: 1301 out of 1563\n",
      "loss: 1.877314805984497, batch: 1351 out of 1563\n",
      "loss: 1.8875327110290527, batch: 1401 out of 1563\n",
      "loss: 1.7094757556915283, batch: 1451 out of 1563\n",
      "loss: 1.8872708082199097, batch: 1501 out of 1563\n",
      "loss: 1.8761639595031738, batch: 1551 out of 1563\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.815474510192871, batch: 1 out of 1563\n",
      "loss: 1.8036088943481445, batch: 51 out of 1563\n",
      "loss: 1.9336133003234863, batch: 101 out of 1563\n",
      "loss: 2.0365843772888184, batch: 151 out of 1563\n",
      "loss: 2.102682113647461, batch: 201 out of 1563\n",
      "loss: 1.9454129934310913, batch: 251 out of 1563\n",
      "loss: 2.0090789794921875, batch: 301 out of 1563\n",
      "loss: 1.8585513830184937, batch: 351 out of 1563\n",
      "loss: 1.917776107788086, batch: 401 out of 1563\n",
      "loss: 2.0060129165649414, batch: 451 out of 1563\n",
      "loss: 1.9094032049179077, batch: 501 out of 1563\n",
      "loss: 1.807187557220459, batch: 551 out of 1563\n",
      "loss: 1.7961454391479492, batch: 601 out of 1563\n",
      "loss: 1.9893373250961304, batch: 651 out of 1563\n",
      "loss: 1.8782768249511719, batch: 701 out of 1563\n",
      "loss: 1.8887956142425537, batch: 751 out of 1563\n",
      "loss: 1.7615175247192383, batch: 801 out of 1563\n",
      "loss: 1.9586189985275269, batch: 851 out of 1563\n",
      "loss: 2.029987335205078, batch: 901 out of 1563\n",
      "loss: 2.024071216583252, batch: 951 out of 1563\n",
      "loss: 1.9168713092803955, batch: 1001 out of 1563\n",
      "loss: 1.8265008926391602, batch: 1051 out of 1563\n",
      "loss: 2.1362736225128174, batch: 1101 out of 1563\n",
      "loss: 1.8500237464904785, batch: 1151 out of 1563\n",
      "loss: 1.9243366718292236, batch: 1201 out of 1563\n",
      "loss: 1.909147024154663, batch: 1251 out of 1563\n",
      "loss: 1.8188660144805908, batch: 1301 out of 1563\n",
      "loss: 1.9349069595336914, batch: 1351 out of 1563\n",
      "loss: 2.070760488510132, batch: 1401 out of 1563\n",
      "loss: 1.836817979812622, batch: 1451 out of 1563\n",
      "loss: 1.9671956300735474, batch: 1501 out of 1563\n",
      "loss: 1.9175989627838135, batch: 1551 out of 1563\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.8106836080551147, batch: 1 out of 1563\n",
      "loss: 1.8650164604187012, batch: 51 out of 1563\n",
      "loss: 1.9039353132247925, batch: 101 out of 1563\n",
      "loss: 1.8745126724243164, batch: 151 out of 1563\n",
      "loss: 1.9128191471099854, batch: 201 out of 1563\n",
      "loss: 1.925365686416626, batch: 251 out of 1563\n",
      "loss: 1.9499142169952393, batch: 301 out of 1563\n",
      "loss: 1.8424464464187622, batch: 351 out of 1563\n",
      "loss: 1.9382660388946533, batch: 401 out of 1563\n",
      "loss: 2.1279985904693604, batch: 451 out of 1563\n",
      "loss: 1.987946629524231, batch: 501 out of 1563\n",
      "loss: 1.8895163536071777, batch: 551 out of 1563\n",
      "loss: 1.9880815744400024, batch: 601 out of 1563\n",
      "loss: 1.9071567058563232, batch: 651 out of 1563\n",
      "loss: 1.8039313554763794, batch: 701 out of 1563\n",
      "loss: 1.8752796649932861, batch: 751 out of 1563\n",
      "loss: 2.0240025520324707, batch: 801 out of 1563\n",
      "loss: 1.8663337230682373, batch: 851 out of 1563\n",
      "loss: 1.9768675565719604, batch: 901 out of 1563\n",
      "loss: 1.8925087451934814, batch: 951 out of 1563\n",
      "loss: 1.8665720224380493, batch: 1001 out of 1563\n",
      "loss: 1.9496405124664307, batch: 1051 out of 1563\n",
      "loss: 1.945433497428894, batch: 1101 out of 1563\n",
      "loss: 1.8762156963348389, batch: 1151 out of 1563\n",
      "loss: 1.950952410697937, batch: 1201 out of 1563\n",
      "loss: 1.764301061630249, batch: 1251 out of 1563\n",
      "loss: 1.7879996299743652, batch: 1301 out of 1563\n",
      "loss: 1.9311754703521729, batch: 1351 out of 1563\n",
      "loss: 1.9375598430633545, batch: 1401 out of 1563\n",
      "loss: 1.853851079940796, batch: 1451 out of 1563\n",
      "loss: 1.8142329454421997, batch: 1501 out of 1563\n",
      "loss: 1.898817539215088, batch: 1551 out of 1563\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 1.947782278060913, batch: 1 out of 1563\n",
      "loss: 2.0914697647094727, batch: 51 out of 1563\n",
      "loss: 1.830321192741394, batch: 101 out of 1563\n",
      "loss: 1.8723517656326294, batch: 151 out of 1563\n",
      "loss: 1.8222081661224365, batch: 201 out of 1563\n",
      "loss: 1.919948697090149, batch: 251 out of 1563\n",
      "loss: 1.9361295700073242, batch: 301 out of 1563\n",
      "loss: 1.929119348526001, batch: 351 out of 1563\n",
      "loss: 1.9954273700714111, batch: 401 out of 1563\n",
      "loss: 1.8326842784881592, batch: 451 out of 1563\n",
      "loss: 1.8952062129974365, batch: 501 out of 1563\n",
      "loss: 1.7885088920593262, batch: 551 out of 1563\n",
      "loss: 1.9554011821746826, batch: 601 out of 1563\n",
      "loss: 1.8448270559310913, batch: 651 out of 1563\n",
      "loss: 1.8709620237350464, batch: 701 out of 1563\n",
      "loss: 1.7309277057647705, batch: 751 out of 1563\n",
      "loss: 1.7207427024841309, batch: 801 out of 1563\n",
      "loss: 1.8894647359848022, batch: 851 out of 1563\n",
      "loss: 1.9872608184814453, batch: 901 out of 1563\n",
      "loss: 2.0386085510253906, batch: 951 out of 1563\n",
      "loss: 1.8745307922363281, batch: 1001 out of 1563\n",
      "loss: 1.9557504653930664, batch: 1051 out of 1563\n",
      "loss: 1.9136077165603638, batch: 1101 out of 1563\n",
      "loss: 1.9551459550857544, batch: 1151 out of 1563\n",
      "loss: 1.8802956342697144, batch: 1201 out of 1563\n",
      "loss: 1.8937203884124756, batch: 1251 out of 1563\n",
      "loss: 1.9089739322662354, batch: 1301 out of 1563\n",
      "loss: 1.7095845937728882, batch: 1351 out of 1563\n",
      "loss: 1.9475245475769043, batch: 1401 out of 1563\n",
      "loss: 1.7988932132720947, batch: 1451 out of 1563\n",
      "loss: 1.9364466667175293, batch: 1501 out of 1563\n",
      "loss: 1.8932604789733887, batch: 1551 out of 1563\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.768323540687561, batch: 1 out of 1563\n",
      "loss: 1.8761831521987915, batch: 51 out of 1563\n",
      "loss: 1.9182497262954712, batch: 101 out of 1563\n",
      "loss: 1.9181092977523804, batch: 151 out of 1563\n",
      "loss: 1.850008249282837, batch: 201 out of 1563\n",
      "loss: 1.8775272369384766, batch: 251 out of 1563\n",
      "loss: 1.8497976064682007, batch: 301 out of 1563\n",
      "loss: 2.0357232093811035, batch: 351 out of 1563\n",
      "loss: 1.991114854812622, batch: 401 out of 1563\n",
      "loss: 1.906617522239685, batch: 451 out of 1563\n",
      "loss: 1.8804949522018433, batch: 501 out of 1563\n",
      "loss: 1.8906328678131104, batch: 551 out of 1563\n",
      "loss: 1.7932134866714478, batch: 601 out of 1563\n",
      "loss: 1.8097457885742188, batch: 651 out of 1563\n",
      "loss: 1.851568341255188, batch: 701 out of 1563\n",
      "loss: 1.881511926651001, batch: 751 out of 1563\n",
      "loss: 1.9226773977279663, batch: 801 out of 1563\n",
      "loss: 2.071223258972168, batch: 851 out of 1563\n",
      "loss: 2.0485644340515137, batch: 901 out of 1563\n",
      "loss: 1.9002567529678345, batch: 951 out of 1563\n",
      "loss: 1.8804510831832886, batch: 1001 out of 1563\n",
      "loss: 1.9064419269561768, batch: 1051 out of 1563\n",
      "loss: 1.748032569885254, batch: 1101 out of 1563\n",
      "loss: 1.83494234085083, batch: 1151 out of 1563\n",
      "loss: 1.9174599647521973, batch: 1201 out of 1563\n",
      "loss: 1.7113146781921387, batch: 1251 out of 1563\n",
      "loss: 2.0692801475524902, batch: 1301 out of 1563\n",
      "loss: 1.7360093593597412, batch: 1351 out of 1563\n",
      "loss: 1.7983007431030273, batch: 1401 out of 1563\n",
      "loss: 1.9135444164276123, batch: 1451 out of 1563\n",
      "loss: 1.8867777585983276, batch: 1501 out of 1563\n",
      "loss: 1.8844325542449951, batch: 1551 out of 1563\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.9987331628799438, batch: 1 out of 1563\n",
      "loss: 1.8904718160629272, batch: 51 out of 1563\n",
      "loss: 1.8283629417419434, batch: 101 out of 1563\n",
      "loss: 1.927839994430542, batch: 151 out of 1563\n",
      "loss: 1.8355696201324463, batch: 201 out of 1563\n",
      "loss: 1.888303279876709, batch: 251 out of 1563\n",
      "loss: 1.7531006336212158, batch: 301 out of 1563\n",
      "loss: 1.798390507698059, batch: 351 out of 1563\n",
      "loss: 1.8304945230484009, batch: 401 out of 1563\n",
      "loss: 1.8760361671447754, batch: 451 out of 1563\n",
      "loss: 1.8520114421844482, batch: 501 out of 1563\n",
      "loss: 1.8739643096923828, batch: 551 out of 1563\n",
      "loss: 2.081430673599243, batch: 601 out of 1563\n",
      "loss: 1.9197624921798706, batch: 651 out of 1563\n",
      "loss: 1.9452918767929077, batch: 701 out of 1563\n",
      "loss: 1.8374357223510742, batch: 751 out of 1563\n",
      "loss: 1.886779546737671, batch: 801 out of 1563\n",
      "loss: 1.9156428575515747, batch: 851 out of 1563\n",
      "loss: 1.8155078887939453, batch: 901 out of 1563\n",
      "loss: 1.94382905960083, batch: 951 out of 1563\n",
      "loss: 1.9034098386764526, batch: 1001 out of 1563\n",
      "loss: 1.945603609085083, batch: 1051 out of 1563\n",
      "loss: 1.7228543758392334, batch: 1101 out of 1563\n",
      "loss: 1.7564637660980225, batch: 1151 out of 1563\n",
      "loss: 1.9285211563110352, batch: 1201 out of 1563\n",
      "loss: 1.911201000213623, batch: 1251 out of 1563\n",
      "loss: 2.0076563358306885, batch: 1301 out of 1563\n",
      "loss: 1.9336549043655396, batch: 1351 out of 1563\n",
      "loss: 2.005462408065796, batch: 1401 out of 1563\n",
      "loss: 1.8419358730316162, batch: 1451 out of 1563\n",
      "loss: 1.9147038459777832, batch: 1501 out of 1563\n",
      "loss: 1.904990315437317, batch: 1551 out of 1563\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 1.877133 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "perf_timer = time.perf_counter()\n",
    "perf_acc = \"\"\n",
    "\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_function, optimizer)\n",
    "    perf_acc = test(test_dataloader, model, loss_function)\n",
    "    \n",
    "print(perf_acc)\n",
    "perf_timer = time.perf_counter() - perf_timer\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"cnn_10\"\n",
    "if True:\n",
    "    torch.save(model.state_dict(), MODELS_PATH + model_name + \".pth\")\n",
    "\n",
    "    with open(MODELS_PATH + model_name + \".txt\", \"w\") as f:\n",
    "        f.write(\"Epochs: {}\\n\".format(EPOCHS))\n",
    "        f.write(\"Batch Size: {}\\n\".format(BATCH_SIZE))\n",
    "        f.write(\"Sample Size: {}\\n\".format(SAMPLE_SIZE))\n",
    "        f.write(\"Feature Set: {}\\n\".format(FEATURES))\n",
    "        f.write(\"Model: {}\\n\".format(str(model)))\n",
    "        f.write(\"Loss Function: {}\\n\".format(\"Cross Entropy Loss\"))\n",
    "        f.write(\"Optimizer: {}\\n\\n\\n\\n\".format(str(optimizer)))\n",
    "        f.write(\"Results: {}\\n\".format(perf_acc))\n",
    "        f.write(\"Timer: {}s\\n\".format(round(perf_timer, 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
