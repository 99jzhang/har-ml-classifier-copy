{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:27: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:27: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/lt/w0169b7x5ml3psz3nly9vj3m0000gn/T/ipykernel_61108/1150052332.py:27: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  def load_data_frame(filename, sep='\\s+', header=None):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top selected features: ['activity', 'gyroJerkY_Std', 'gyroJerkY_Rms', 'gyroJerkX_Rms', 'gyroJerkX_Std', 'gyroJerkZ_Std', 'gyroJerkZ_Rms', 'gravityAccY_Std']\n",
      "(6108, 1)\n",
      "1\n",
      "1\n",
      "(1527, 1)\n",
      "1\n",
      "1\n",
      "(3273, 1)\n",
      "1\n",
      "1\n",
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lt/w0169b7x5ml3psz3nly9vj3m0000gn/T/ipykernel_61108/1150052332.py:27: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  def load_data_frame(filename, sep='\\s+', header=None):\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 8, got 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 264\u001b[0m\n\u001b[1;32m    261\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 264\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 187\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m epoch_val_loss, epoch_val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model, optimizer,scheduler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(models, optimizers,schedulers):\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_single_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m     epoch_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\n\u001b[1;32m    189\u001b[0m     epoch_train_accuracy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_accuracy\n",
      "Cell \u001b[0;32mIn[9], line 89\u001b[0m, in \u001b[0;36mtrain_single_model\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     87\u001b[0m X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(DEVICE), y\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     88\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 89\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, y)\n\u001b[1;32m     91\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 69\u001b[0m, in \u001b[0;36mGRUNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 69\u001b[0m     x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# x shape will be (batch_size, seq_len, hidden_size * 2)\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     x \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# x shape will be (batch_size, hidden_size * 2)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norm1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1137\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m-> 1137\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1139\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m   1140\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/torch/nn/modules/rnn.py:280\u001b[0m, in \u001b[0;36mRNNBase.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, hidden: Tensor, batch_sizes: Optional[Tensor]):\n\u001b[0;32m--> 280\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m     expected_hidden_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden, expected_hidden_size)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/torch/nn/modules/rnn.py:246\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 8, got 1"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.neighbors import NeighborhoodComponentsAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Constants\n",
    "DATASET_PATH = 'NewGeneratedData/'\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 20\n",
    "NUM_FEATURES = 8\n",
    "NUM_MODELS = 6  # Number of models in the ensemble \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "WEIGHT_DECAY = 1e-4 # L2 Regularization parameter\n",
    "\n",
    "# Utility Functions\n",
    "def load_data_frame(filename, sep='\\s+', header=None):\n",
    "    filepath = os.path.join(DATASET_PATH, filename + '.txt')\n",
    "    return pd.read_csv(filepath, header=header, sep=sep)\n",
    "\n",
    "# Dataset class\n",
    "class HAPTDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        total_features = features.shape[1]\n",
    "        seq_len_options = [1]  # Possible sequence lengths\n",
    "        seq_len = next((seq for seq in seq_len_options if total_features % seq == 0), 1)\n",
    "        input_size = total_features // seq_len\n",
    "        self.data = torch.tensor(features, dtype=torch.float32).view(-1, seq_len, input_size)\n",
    "        self.labels = torch.tensor(labels.values - 1, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# GRU Network class\n",
    "class GRUNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, hidden_size=64, num_layers=2):\n",
    "        super(GRUNetwork, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=0.1, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, 256)  # Multiply by 2 for bidirectional\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(256)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(128)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(64)\n",
    "        self.batch_norm4 = nn.BatchNorm1d(32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru(x)  # x shape will be (batch_size, seq_len, hidden_size * 2)\n",
    "        x = x[:, -1, :]  # x shape will be (batch_size, hidden_size * 2)\n",
    "        x = self.relu(self.batch_norm1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.leaky_relu(self.batch_norm2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.leaky_relu(self.batch_norm3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.batch_norm4(self.fc4(x)))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "# Training function for one model\n",
    "def train_single_model(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = loss_fn(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / len(dataloader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Testing function for one model\n",
    "def test_single_model(dataloader, model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "            output = model(X)\n",
    "            loss = loss_fn(output, y)\n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / len(dataloader.dataset)\n",
    "    return accuracy, avg_loss, all_preds, all_labels\n",
    "\n",
    "# Main Process\n",
    "def main():\n",
    "    features = load_data_frame(\"features\")[0].to_list()\n",
    "\n",
    "    complete_set = load_data_frame(\"har_dataset\")\n",
    "    complete_set.columns = features\n",
    "    train_set, test_set = train_test_split(complete_set, test_size=0.3, random_state=37)\n",
    "\n",
    "    train_set['activity'] = train_set['activity'].astype(int) - 1\n",
    "    test_set['activity'] = test_set['activity'].astype(int) - 1\n",
    "\n",
    "    activity_labels = load_data_frame('activity_labels')\n",
    "    activity_labels.columns = ['id', 'activity']\n",
    "    activity_labels['id'] = activity_labels['id'].astype(int) - 1\n",
    "    \n",
    "    # Feature Selection using NCA\n",
    "    X_train, X_val, y_train, y_val = train_test_split(train_set[features], train_set['activity'], test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale Data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(test_set[features])\n",
    "    \n",
    "    # # NCA feature selection\n",
    "    nca = NeighborhoodComponentsAnalysis(n_components=NUM_FEATURES, random_state=42)\n",
    "    X_train_scaled = nca.fit_transform(X_train_scaled, y_train)\n",
    "    X_val_scaled = nca.transform(X_val_scaled)\n",
    "    X_test_scaled = nca.transform(X_test_scaled)\n",
    "    important_features = np.abs(nca.components_).sum(axis=0)\n",
    "    sorted_indices = np.argsort(important_features)[::-1]  # Sort in descending order\n",
    "\n",
    "    # Get the feature names corresponding to the sorted indices\n",
    "    selected_features = [features[i] for i in sorted_indices[:NUM_FEATURES]]\n",
    "    print(\"Top selected features:\", selected_features)\n",
    "\n",
    "    # Prepare datasets\n",
    "    train_dataset = HAPTDataset(X_train_scaled, y_train)\n",
    "    val_dataset = HAPTDataset(X_val_scaled, y_val)\n",
    "    test_dataset = HAPTDataset(X_test_scaled, test_set['activity'])\n",
    "\n",
    "    # DataLoaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Create ensemble of models\n",
    "    models = [GRUNetwork(input_size=NUM_FEATURES, num_classes=12).to(DEVICE) for _ in range(NUM_MODELS)]\n",
    "    optimizers = [optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY) for model in models]\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses, train_accuracies = [], []\n",
    "    val_losses, val_accuracies = [], []\n",
    "    schedulers = [StepLR(optimizer, step_size=10, gamma=0.1) for optimizer in optimizers]\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "        epoch_train_loss, epoch_train_accuracy = 0, 0\n",
    "        epoch_val_loss, epoch_val_accuracy = 0, 0\n",
    "        \n",
    "        for model, optimizer,scheduler in zip(models, optimizers,schedulers):\n",
    "            # Train the model\n",
    "            train_loss, train_accuracy = train_single_model(train_dataloader, model, loss_function, optimizer)\n",
    "            epoch_train_loss += train_loss\n",
    "            epoch_train_accuracy += train_accuracy\n",
    "            scheduler.step()\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        for model in models:\n",
    "            val_accuracy, val_loss, _, _ = test_single_model(val_dataloader, model)\n",
    "            epoch_val_loss += val_loss\n",
    "            epoch_val_accuracy += val_accuracy\n",
    "\n",
    "        # Average out losses and accuracies\n",
    "        avg_train_loss = epoch_train_loss / NUM_MODELS\n",
    "        avg_train_accuracy = epoch_train_accuracy / NUM_MODELS\n",
    "        avg_val_loss = epoch_val_loss / NUM_MODELS\n",
    "        avg_val_accuracy = epoch_val_accuracy / NUM_MODELS\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(avg_train_accuracy)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(avg_val_accuracy)\n",
    "\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {avg_train_accuracy * 100:.2f}%\")\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {avg_val_accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Evaluate ensemble models on test data\n",
    "    test_accuracies = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    test_losses = []\n",
    "\n",
    "    for model in models:\n",
    "        accuracy, loss, preds, labels = test_single_model(test_dataloader, model)\n",
    "        test_accuracies.append(accuracy)\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "        test_losses.append(loss)\n",
    "\n",
    "    avg_test_accuracy = np.mean(test_accuracies)\n",
    "    avg_test_loss = np.mean(test_losses)\n",
    "    print(f\"\\nTest Accuracy of Ensemble: {avg_test_accuracy * 100:.2f}%\")\n",
    "    print(f\"Test Loss of Ensemble: {avg_test_loss:.2f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Normalize the confusion matrix to percentages\n",
    "    conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "    # Plotting the normalized confusion matrix\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(conf_matrix_normalized, annot=True, fmt='.2f', cmap='Blues')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix (in %)')\n",
    "    plt.show()\n",
    "    # Plotting training and validation loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, EPOCHS + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, EPOCHS + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting training and validation accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, EPOCHS + 1), train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(range(1, EPOCHS + 1), val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
